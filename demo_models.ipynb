{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define the BERT encoder\n",
    "encoder_inputs = Input(shape=(max_sequence_length,), dtype=tf.int32, name='encoder_inputs')\n",
    "bert_output = bert_model(encoder_inputs)\n",
    "encoder_embedding = bert_output.last_hidden_state\n",
    "\n",
    "# Use LSTM layers after BERT embeddings\n",
    "encoder_lstm1 = Bidirectional(LSTM(96, return_sequences=True, name='encoder_lstm_1', dropout=0.3,\n",
    "                                    recurrent_dropout=0.3, kernel_regularizer=l2(1e-4)))(encoder_embedding)\n",
    "encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(96, return_sequences=True, return_state=True,\n",
    "                                                    name='encoder_lstm_2', dropout=0.3, recurrent_dropout=0.3,\n",
    "                                                    kernel_regularizer=l2(1e-4)))(encoder_lstm1)\n",
    "\n",
    "state_h = Concatenate()([forward_h2, backward_h2])\n",
    "state_c = Concatenate()([forward_c2, backward_c2])\n",
    "\n",
    "# Decoder setup remains the same\n",
    "decoder_inputs = Input(shape=(21, y_max_sequence_length,), name='decoder_inputs')\n",
    "decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
    "                                              trainable=False))(decoder_inputs)\n",
    "\n",
    "decoder_outputs = []\n",
    "for i in range(21):\n",
    "    decoder_lstm_1 = LSTM(96 * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
    "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
    "    decoder_lstm_2 = LSTM(96 * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
    "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
    "    decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
    "    decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
    "    decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
    "    decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.004872)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, cooldown=0)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize and encode inputs using BERT\n",
    "def encode_with_bert(texts):\n",
    "    inputs = bert_tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Define the custom Keras layer to use BERT embeddings\n",
    "class BertEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEncoderLayer, self).__init__(**kwargs)\n",
    "        self.bert_model = tf.keras.layers.Lambda(lambda x: encode_with_bert(x))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.bert_model(inputs)\n",
    "\n",
    "# Define the custom Keras layer to use GPT-2\n",
    "class Gpt2DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Gpt2DecoderLayer, self).__init__(**kwargs)\n",
    "        self.gpt2_model = gpt2_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.gpt2_model(inputs)[0]\n",
    "\n",
    "# Define input layers\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.string, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "\n",
    "# Encode with BERT\n",
    "bert_encoder = BertEncoderLayer()(encoder_inputs)\n",
    "\n",
    "# Decode with GPT-2\n",
    "gpt2_decoder = Gpt2DecoderLayer()(decoder_inputs)\n",
    "\n",
    "# Define output layer\n",
    "outputs = Dense(gpt2_tokenizer.vocab_size, activation='softmax')(gpt2_decoder)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, TimeDistributed, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize and encode inputs using BERT\n",
    "def encode_with_bert(texts):\n",
    "    inputs = bert_tokenizer(texts, return_tensors='tf', padding=True, truncation=True)\n",
    "    outputs = bert_model(inputs.input_ids)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Define the custom Keras layer to use BERT embeddings\n",
    "class BertEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEncoderLayer, self).__init__(**kwargs)\n",
    "        self.bert_model = tf.keras.layers.Lambda(lambda x: encode_with_bert(x))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.bert_model(inputs)\n",
    "\n",
    "# Define the custom Keras layer to use GPT-2\n",
    "class Gpt2DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Gpt2DecoderLayer, self).__init__(**kwargs)\n",
    "        self.gpt2_model = gpt2_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.gpt2_model(inputs)[0]\n",
    "\n",
    "# Define input layers\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.string, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(21, None), dtype=tf.string, name='decoder_inputs')\n",
    "\n",
    "# Encode with BERT\n",
    "bert_encoder = BertEncoderLayer()(encoder_inputs)\n",
    "\n",
    "# Process each of the 21 sequences separately\n",
    "decoder_outputs = []\n",
    "for i in range(21):\n",
    "    gpt2_decoder = Gpt2DecoderLayer()(decoder_inputs[:, i])\n",
    "    decoder_dense = Dense(gpt2_tokenizer.vocab_size, activation='softmax')(gpt2_decoder)\n",
    "    decoder_outputs.append(decoder_dense)\n",
    "\n",
    "# Stack decoder outputs\n",
    "decoder_outputs = tf.stack(decoder_outputs, axis=1)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize inputs using BERT tokenizer\n",
    "def bert_tokenize(texts):\n",
    "    inputs = bert_tokenizer.batch_encode_plus(texts.numpy().tolist(), return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "# Tokenize inputs using GPT-2 tokenizer\n",
    "def gpt2_tokenize(texts):\n",
    "    inputs = gpt2_tokenizer.batch_encode_plus(texts.numpy().tolist(), return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    return inputs['input_ids']\n",
    "\n",
    "# Define the custom Keras layer to use BERT embeddings\n",
    "class BertEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEncoderLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = tf.py_function(func=bert_tokenize, inp=[inputs], Tout=[tf.int32, tf.int32])\n",
    "        input_ids.set_shape([None, None])\n",
    "        attention_mask.set_shape([None, None])\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "        return outputs\n",
    "\n",
    "# Define the custom Keras layer to use GPT-2\n",
    "class Gpt2DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Gpt2DecoderLayer, self).__init__(**kwargs)\n",
    "        self.gpt2_model = gpt2_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tokenized_inputs = tf.py_function(func=gpt2_tokenize, inp=[inputs], Tout=tf.int32)\n",
    "        tokenized_inputs.set_shape([None, None])\n",
    "        outputs = self.gpt2_model(tokenized_inputs).logits\n",
    "        return outputs\n",
    "\n",
    "# Define input layers\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.string, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(21, None,), dtype=tf.string, name='decoder_inputs')\n",
    "\n",
    "# Encode with BERT\n",
    "bert_encoder = BertEncoderLayer()(encoder_inputs)\n",
    "\n",
    "# Process each of the 21 sequences separately\n",
    "decoder_outputs = []\n",
    "for i in range(21):\n",
    "    gpt2_decoder = Gpt2DecoderLayer()(decoder_inputs[:, i])\n",
    "    decoder_dense = TimeDistributed(Dense(gpt2_tokenizer.vocab_size, activation='softmax'))(gpt2_decoder)\n",
    "    decoder_outputs.append(decoder_dense)\n",
    "\n",
    "# Stack decoder outputs\n",
    "decoder_outputs = Lambda(lambda x: tf.stack(x, axis=1))(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define BERT encoding function\n",
    "def encode_with_bert(texts):\n",
    "    inputs = bert_tokenizer.batch_encode_plus(texts.numpy().tolist(), return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    outputs = bert_model(inputs['input_ids'])\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Define GPT-2 tokenization function\n",
    "def tokenize_gpt2(texts):\n",
    "    inputs = gpt2_tokenizer.batch_encode_plus(texts.numpy().tolist(), return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    return inputs['input_ids']\n",
    "\n",
    "# Custom Keras layer to use BERT embeddings\n",
    "class BertEncoderLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertEncoderLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids = tf.py_function(func=encode_with_bert, inp=[inputs], Tout=tf.float32)\n",
    "        input_ids.set_shape([None, None, 768])  # Set shape to BERT's output\n",
    "        return input_ids\n",
    "\n",
    "# Custom Keras layer to use GPT-2\n",
    "class Gpt2DecoderLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Gpt2DecoderLayer, self).__init__(**kwargs)\n",
    "        self.gpt2_model = gpt2_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tokenized_inputs = tf.py_function(func=tokenize_gpt2, inp=[inputs], Tout=tf.int32)\n",
    "        tokenized_inputs.set_shape([None, None])\n",
    "        outputs = self.gpt2_model(tokenized_inputs).logits\n",
    "        return outputs\n",
    "\n",
    "# Define input layers\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.string, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(21, None,), dtype=tf.string, name='decoder_inputs')\n",
    "\n",
    "# Encode with BERT\n",
    "bert_encoder = BertEncoderLayer()(encoder_inputs)\n",
    "\n",
    "# Efficiently process each of the 21 sequences separately\n",
    "def process_decoder_sequence(i, decoder_inputs, gpt2_model):\n",
    "    gpt2_decoder = Gpt2DecoderLayer()(decoder_inputs[:, i])\n",
    "    decoder_dense = Dense(gpt2_tokenizer.vocab_size, activation='softmax')(gpt2_decoder)\n",
    "    return decoder_dense\n",
    "\n",
    "decoder_outputs = []\n",
    "for i in range(21):\n",
    "    decoder_output = process_decoder_sequence(i, decoder_inputs, gpt2_model)\n",
    "    decoder_outputs.append(decoder_output)\n",
    "\n",
    "# Stack decoder outputs\n",
    "decoder_outputs = Lambda(lambda x: tf.stack(x, axis=1))(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Encode input sequences\n",
    "def encode_text(texts):\n",
    "    return t5_tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Define the model\n",
    "def generate_text(inputs):\n",
    "    inputs = encode_text(inputs)\n",
    "    outputs = t5_model.generate(inputs['input_ids'])\n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "input_texts = [\"Translate English to French: How are you?\", \"Summarize this text.\"]\n",
    "print(generate_text(input_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
