{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-to-Many Sequence Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One-to-many sequence problems are the type of sequence problems where input data has one time-step and the output contains a vector of multiple\n",
    "values or multiple time-steps.\n",
    "\"\"\"\n",
    "\n",
    "X = list()\n",
    "Y = list()\n",
    "X = [x+3 for x in range(-2, 43, 3)]\n",
    "\n",
    "for i in X:\n",
    "    output_vector = list()\n",
    "    output_vector.append(i+1)\n",
    "    output_vector.append(i+2)\n",
    "    Y.append(output_vector)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "\"\"\"\n",
    "[1, 4, 7, . . . . . ]\n",
    "[[2, 3], [5, 6], [8, 9], . . . . ]\n",
    "\n",
    "input contains 15 samples with one time-step and one feature value. For each value in the input sample, the corresponding output vector\n",
    "contains the next two integers. For instance, if the input is 4, the output vector will contain values 5 and 6.\n",
    "\"\"\"\n",
    "X = np.array(X).reshape(15, 1, 1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(1, 1)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=1000, validation_split=0.2, batch_size=3)\n",
    "\n",
    "test_input = array([10])\n",
    "test_input = test_input.reshape((1, 1, 1))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 1)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 1)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-to-Many Sequence Problems with Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = 25\n",
    "\n",
    "X1 = list()\n",
    "X2 = list()\n",
    "X = list()\n",
    "Y = list()\n",
    "\n",
    "X1 = [(x+1)*2 for x in range(25)]\n",
    "X2 = [(x+1)*3 for x in range(25)]\n",
    "\n",
    "for x1, x2 in zip(X1, X2):\n",
    "    output_vector = list()\n",
    "    output_vector.append(x1+1)\n",
    "    output_vector.append(x2+1)\n",
    "    Y.append(output_vector)\n",
    "\n",
    "X = np.column_stack((X1, X2))\n",
    "print(X)\n",
    "\n",
    "\"\"\"\n",
    "[[ 2  3]\n",
    " [ 4  6]\n",
    " [ 6  9]\n",
    " .\n",
    " .]\n",
    "\n",
    "input time-step consists of two features. The output will be a vector which contains the next two elements that correspond to \n",
    "the two features in the time-step of the input sample\n",
    "\"\"\"\n",
    "\n",
    "X = np.array(X).reshape(25, 1, 2)\n",
    "Y = np.array(Y)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(1, 2)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, Y, epochs=1000, validation_split=0.2, batch_size=3)\n",
    "\n",
    "test_input = array([40, 60])\n",
    "test_input = test_input.reshape((1, 1, 2))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 2)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "\n",
    "test_input = array([40, 60])\n",
    "test_input = test_input.reshape((1, 1, 2))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 2)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To solve such sequence problems, the encoder-decoder model has been designed. The encoder-decoder model is basically a fancy name for \n",
    "neural network architecture with two LSTM layers.\n",
    "The first layer works as an encoder layer and encodes the input sequence. The decoder is also an LSTM layer, which accepts three inputs: \n",
    "the encoded sequence from the encoder LSTM, the previous hidden state, and the current input. During training the actual output at \n",
    "each time-step is used to train the encoder-decoder model. While making predictions, the encoder output, the current hidden state, \n",
    "and the previous output is used as input to make predictions at each time-step. These concepts will become more understandable \n",
    "when you will see them in action in an upcoming section.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-Many Sequence Problems with Single Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "Y = list()\n",
    "X = [x for x in range(5, 301, 5)]\n",
    "Y = [y for y in range(20, 316, 5)]\n",
    "\n",
    "X = np.array(X).reshape(20, 3, 1)\n",
    "Y = np.array(Y).reshape(20, 3, 1)\n",
    "\n",
    "\"\"\"\n",
    "The input X contains 20 samples where each sample contains 3 time-steps with one feature.\n",
    "[[[  5]\n",
    "  [ 10]\n",
    "  [ 15]]\n",
    "You can see that the input sample contains 3 values that are basically 3 consecutive multiples of 5.\n",
    "[[[ 20]\n",
    "  [ 25]\n",
    "  [ 30]]\n",
    "\n",
    "The output contains the next three consecutive multiples of 5.\n",
    "\n",
    "For the encoder-decoder model, the output should also be converted into a 3D format containing the number of samples, time-steps, and features. \n",
    "This is because the decoder generates an output per time-step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# encoder layer\n",
    "model.add(LSTM(100, activation='relu', input_shape=(3, 1)))\n",
    "\n",
    "# repeat vector\n",
    "model.add(RepeatVector(3))\n",
    "\n",
    "# decoder layer\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "first LSTM layer is the encoder layer.we have added the repeat vector to our model. The repeat vector takes the output from the encoder and \n",
    "feeds it repeatedly as input at each time-step to the decoder. For instance, in the output we have three time-steps. To predict each output \n",
    "time-step, the decoder will use the value from the repeat vector, the hidden state from the previous output and the current input.\n",
    "\n",
    "Next we have a decoder layer. Since the output is in the form of a time-step, which is a 3D format, the return_sequences for the decoder \n",
    "model has been set True. The TimeDistributed layer is used to individually predict the output for each time-step.\n",
    "\"\"\"\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "\n",
    "test_input = array([300, 305, 310])\n",
    "test_input = test_input.reshape((1, 3, 1))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', input_shape=(3, 1))))\n",
    "model.add(RepeatVector(3))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-Many Sequence Problems with Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in many-to-many sequence problems, each time-step in the input sample contains multiple features.\n",
    "\"\"\"\n",
    "\n",
    "X = list()\n",
    "Y = list()\n",
    "X1 = [x1 for x1 in range(5, 301, 5)]\n",
    "X2 = [x2 for x2 in range(20, 316, 5)]\n",
    "Y = [y for y in range(35, 331, 5)]\n",
    "\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "\"\"\"\n",
    "In the script above we create two lists X1 and X2. The list X1 contains all the multiples of 5 from 5 to 300 (inclusive) and the list X2 \n",
    "contains all the multiples of 5 from 20 to 315 (inclusive). Finally, the list Y, which happens to be the output contains all the multiples \n",
    "of 5 between 35 and 330 (inclusive). The final input list X is a column-wise merger of X1 and X2.\n",
    "\"\"\"\n",
    "\n",
    "X = np.array(X).reshape(20, 3, 2)\n",
    "Y = np.array(Y).reshape(20, 3, 1)\n",
    "\n",
    "\"\"\"\n",
    "You can see the input X has been reshaped into 20 samples of three time-steps with 2 features where the output has been reshaped \n",
    "into similar dimensions but with 1 feature.\n",
    "\n",
    "[[ 5  20]\n",
    "[ 10  25]\n",
    "[ 15  30]]\n",
    "\n",
    "The input contains 6 consecutive multiples of integer 5, three each in the two columns.\n",
    "[[ 35]\n",
    "[ 40]\n",
    "[ 45]]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The following script trains the stacked LSTM model. You can see that the input shape is now (3, 2) corresponding to three time-steps \n",
    "and two features in the input.\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(3, 2)))\n",
    "model.add(RepeatVector(3))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)\n",
    "\n",
    "X1 = [300, 305, 310]\n",
    "X2 = [315, 320, 325]\n",
    "\n",
    "test_input = np.column_stack((X1, X2))\n",
    "\n",
    "test_input = test_input.reshape((1, 3, 2))\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', input_shape=(3, 2))))\n",
    "model.add(RepeatVector(3))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
    "                              input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, name='encoder_lstm', dropout=0.5, recurrent_dropout=0.5))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Concatenate forward and backward states\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
    "decoder_embedding = TimeDistributed(Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
    "                                              trainable=False))(decoder_inputs)\n",
    "\n",
    "decoder_outputs = []\n",
    "for i in range(31):\n",
    "    decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_{i}', dropout=0.5, recurrent_dropout=0.5)\n",
    "    decoder_lstm_output = decoder_lstm(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
    "    decoder_dense = TimeDistributed(Dense(vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
    "    decoder_outputs.append(decoder_dense(decoder_lstm_output))\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
    "                              input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
    "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, name='encoder_lstm', dropout=0.5, recurrent_dropout=0.5))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Concatenate forward and backward states\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
    "decoder_embedding = TimeDistributed(Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
    "                                              trainable=False))(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_{i}', dropout=0.5, recurrent_dropout=0.5)\n",
    "decoder_lstm_output = decoder_lstm(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
    "decoder_outputs.append(decoder_dense(decoder_lstm_output))\n",
    "\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
