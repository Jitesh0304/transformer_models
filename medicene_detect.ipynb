{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item Name</th>\n",
       "      <th>Item Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUMALOG MIX 50 CARTRIDGE, ZECAL FEM TABLET</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIFENAC P TABLETS</td>\n",
       "      <td>HIFENAC P : 1 TABLETS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Item Name             Item Count\n",
       "0  HUMALOG MIX 50 CARTRIDGE, ZECAL FEM TABLET                    nan\n",
       "1                           HIFENAC P TABLETS  HIFENAC P : 1 TABLETS"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r'C:\\Users\\jites\\Desktop\\Project_folder\\Medicine_and_count_multiple_sentences.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df.astype(str)\n",
    "df = df.fillna(\"\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jites\\anaconda3\\envs\\torchenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2-finetuned-model-medicine-1').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-finetuned-tokenizer-medicine-1')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Regularization: Add dropout and weight decay\n",
    "model.config.attn_pdrop = 0.2\n",
    "model.config.embd_pdrop = 0.2\n",
    "model.config.resid_pdrop = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'HAPPIBIOTIC CAP, MAXILIV INJECTION, DAPEFY S 10100MG TABLET => MAXILIV : 1 INJECTION'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(df):\n",
    "    texts = df['Item Name'].tolist()\n",
    "    # labels = [' '.join([f\"{label}: {sentence}\" for label, sentence in zip(df.columns[:-1], row[:-1])]) for row in df.itertuples(index=False)]\n",
    "    labels = df['Item Count'].tolist()\n",
    "    \n",
    "    # Create a list of dictionaries\n",
    "    data = [{'text': f\"{text} => {label}\"} for text, label in zip(texts, labels)]\n",
    "    return data\n",
    "\n",
    "xx = create_dataset(df)\n",
    "xx[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850421b7e70c4f45a2d07733f9ea46f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d6717068514f18aad4add9699fbd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_dataset(df):\n",
    "    texts = df['Item Name'].tolist()\n",
    "    # labels = [' '.join([f\"{label}: {sentence}\" for label, sentence in zip(df.columns[:-1], row[:-1])]) for row in df.itertuples(index=False)]\n",
    "    labels = df['Item Count'].tolist()\n",
    "    \n",
    "    # Create a list of dictionaries\n",
    "    data = [{'text': f\"{text} => {label}\"} for text, label in zip(texts, labels)]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# Create Hugging Face datasets from DataFrames\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     tokenizer=tokenizer, padding=True, model= model\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "#### Optimizer and learning rate scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=500, eta_min=5e-6)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.001)       ## weight_decay =>  L2 regularization\n",
    "scheduler = ReduceLROnPlateau(optimizer, min_lr=7e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcc2a522bcc492088928bf04b462f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,      ## 2\n",
    "    gradient_accumulation_steps=4,  # Accumulate over 4 batches to simulate a batch size of 32\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # eval_strategy=\"steps\",\n",
    "    # save_strategy='steps',\n",
    "    weight_decay=0.001,  ## L2 regularization\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,  # Load best model at the end based on evaluation\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    # fp16=True,  # Enable mixed precision\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=repo_name,\n",
    "#     group_by_length=True,\n",
    "#     length_column_name='input_length',\n",
    "#     per_device_train_batch_size=24,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     num_train_epochs=20,\n",
    "#     fp16=True,\n",
    "#     save_steps=1000,\n",
    "#     save_strategy='steps', # we cannot set it to \"no\". Otherwise, the model cannot guess the best checkpoint.\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=1000,\n",
    "#     learning_rate=5e-5,\n",
    "#     warmup_steps=500,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end = True # this will let the model save the best checkpoint\n",
    "# )\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    # train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained('./gpt2-finetuned-model-medicine-2')\n",
    "tokenizer.save_pretrained('./gpt2-finetuned-tokenizer-medicine-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2-finetuned-model-medicine-2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-finetuned-tokenizer-medicine-2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = model.half() if torch.cuda.is_available() else model\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def generate_labels(sent):\n",
    "    paragraph = sent\n",
    "    input_text = f\"{paragraph} =>\"\n",
    "\n",
    "    # Encode with truncation and padding\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=256,  # Adjust based on your requirement\n",
    "        truncation=True,  # Explicit truncation\n",
    "        padding='max_length'  # Padding to the max length\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Generate predictions with optimized settings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=512,  # Adjust based on your requirement\n",
    "            num_return_sequences=1,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Process and return the output\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    labels = output_text.split('=>')[-1].strip()\n",
    "    return labels\n",
    "\n",
    "\n",
    "result = generate_labels(\"HAPPIBIOTIC CAP, MAXILIV INJECTION, DAPEFY S 10100MG TABLET\")\n",
    "\n",
    "def parse_output(output):\n",
    "    lines = output.split(',')\n",
    "    labeled_sentences = {}\n",
    "    current_label = None\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            current_label, sentence = line.split(':', 1)\n",
    "            if current_label.strip() in labeled_sentences:\n",
    "                pass\n",
    "            else:\n",
    "                labeled_sentences[current_label.strip()] = sentence.strip()\n",
    "        else:\n",
    "            if current_label:\n",
    "                if current_label.strip() in labeled_sentences:\n",
    "                    pass\n",
    "                else:\n",
    "                    labeled_sentences[current_label] += ' ' + line.strip()\n",
    "    return labeled_sentences\n",
    "\n",
    "dict_data = parse_output(result)\n",
    "print(dict_data)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
