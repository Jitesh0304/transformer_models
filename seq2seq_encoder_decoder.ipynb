{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NzyquC_cUaf"
      },
      "outputs": [],
      "source": [
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-44jT1LFJ8WD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path = '/content/FInal_data_20_columns.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "df = df.astype(str)\n",
        "# df = df.fillna(\"\")\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g4ga2JKPYnB"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NAwpEeyLnXhg"
      },
      "outputs": [],
      "source": [
        "# Function to check and format text\n",
        "def check_text(text):\n",
        "    if text in ['Not defined']:\n",
        "        return 'NaN'\n",
        "    return text\n",
        "\n",
        "# Create the paragraph column\n",
        "for col in df.columns:\n",
        "  df[col] = df[col].apply(lambda txt: check_text(txt))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prHPBq9-Pfv0"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PuvzGQipM4n"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4v4H2kdtR14"
      },
      "outputs": [],
      "source": [
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "df = df.applymap(lambda x: lower_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZERHNmbKFfs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKIQx6QrKFmD"
      },
      "outputs": [],
      "source": [
        "text_data = df['Paragraph']\n",
        "target_data = df.drop(columns=['Paragraph'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6Fqk8FQKFqw"
      },
      "outputs": [],
      "source": [
        "print(text_data.shape)\n",
        "print(target_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70v8_zbmLeH_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Concatenate, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0FHI8ayZaNT"
      },
      "outputs": [],
      "source": [
        "print(max([len(sen.split()) for sen in text_data]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuHYTgz-KFvE"
      },
      "outputs": [],
      "source": [
        "max_words_in_sentense = max([len(sen.split()) for sen in text_data])\n",
        "tokenizer = Tokenizer(num_words=max_words_in_sentense)\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "# word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gjvymQNSQIj"
      },
      "outputs": [],
      "source": [
        "# special_tokens = ['<start>', '<end>']\n",
        "# for token in special_tokens:\n",
        "#     tokenizer.word_index[token] = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "tokenizer.word_index['<start>'] = 0\n",
        "tokenizer.index_word[0] = '<start>'\n",
        "\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNC1HjjTSkOp"
      },
      "outputs": [],
      "source": [
        "start_token_id = word_index['<start>']\n",
        "# end_token_id = word_index['<end>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37qPukEV1qOW"
      },
      "outputs": [],
      "source": [
        "# word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iGc0UMoq8Z1"
      },
      "outputs": [],
      "source": [
        "# text_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcuRbcBXKGOH"
      },
      "outputs": [],
      "source": [
        "tokenizer.index_word = {index: word for word, index in word_index.items()}\n",
        "sequences = tokenizer.texts_to_sequences(text_data)\n",
        "# max_sequence_length = 700\n",
        "max_sequence_length = max_words_in_sentense\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro8vL9h26LAt"
      },
      "outputs": [],
      "source": [
        "padded_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFTAeaQM6Fhd"
      },
      "outputs": [],
      "source": [
        "vocab_size_input = len(tokenizer.word_counts)\n",
        "print(vocab_size_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9qv8WnqzxRt"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6YK2Ru4y3Y4"
      },
      "outputs": [],
      "source": [
        "# padded_sequences[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e8ylFIfKGaD"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_file_path = '/content/drive/MyDrive/glove.6B.50d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_file_path)\n",
        "print(f'Loaded {len(embeddings_index)} word vectors.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6_Ne-3-zPV7"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "embedding_matrix_input = np.zeros((vocab_size_input, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size_input:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix_input[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1DeToazYvT"
      },
      "outputs": [],
      "source": [
        "# embedding_matrix[40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZe7Omw_6g2v"
      },
      "outputs": [],
      "source": [
        "# Separate tokenizer for target data\n",
        "tokenizer_target = Tokenizer(num_words= max_words_in_sentense)\n",
        "tokenizer_target.fit_on_texts(target_data.values.flatten())\n",
        "word_index_target = tokenizer_target.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmubJZ7b7rCy"
      },
      "outputs": [],
      "source": [
        "latent_dim = 32\n",
        "x_max_sequence_length = max_words_in_sentense\n",
        "# y_max_sequence_length = max_words_in_sentense\n",
        "y_max_sequence_length = 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwzNHV-_KGhi"
      },
      "outputs": [],
      "source": [
        "# target_sequences = np.zeros((len(df), target_data.shape[1], max_sequence_length), dtype='int32')\n",
        "\n",
        "# for i, (key, sentences) in enumerate(target_data.items()):\n",
        "#     tokenizer.fit_on_texts(sentences)\n",
        "#     sequences_y = tokenizer.texts_to_sequences(sentences)\n",
        "#     padded_sequences_y = pad_sequences(sequences_y, maxlen=max_sequence_length, padding='post')\n",
        "#     target_sequences[:, i, :] = padded_sequences_y\n",
        "\n",
        "\n",
        "target_sequences = np.zeros((len(df), target_data.shape[1], y_max_sequence_length), dtype='int32')\n",
        "for i, (key, sentences) in enumerate(target_data.items()):\n",
        "    sequences_target = tokenizer_target.texts_to_sequences(sentences)\n",
        "    padded_sequences_target = pad_sequences(sequences_target, maxlen=y_max_sequence_length, padding='post')\n",
        "    target_sequences[:, i, :] = padded_sequences_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VahV3_ZpX-NR"
      },
      "outputs": [],
      "source": [
        "# Assuming your model and data preparation are set up as previously discussed\n",
        "\n",
        "# Splitting data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, target_sequences, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEeNd69vX8b9"
      },
      "outputs": [],
      "source": [
        "y_vocab_size_input = len(tokenizer.word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kDn45vlSzeJ"
      },
      "outputs": [],
      "source": [
        "def add_start_token(sequences, start_token_id):\n",
        "    return np.array([[start_token_id] + seq for seq in sequences])\n",
        "\n",
        "y_train = add_start_token(y_train, start_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLhXUki8X-Yr"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzmZ5ll9cvNC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, TimeDistributed, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "        ## model 1\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(12, max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, trainable=False)(decoder_inputs)\n",
        "\n",
        "# decoder_outputs = []\n",
        "# for i in range(target_data.shape[1]):\n",
        "#     decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=False, name=f'decoder_lstm_{i}')\n",
        "#     decoder_lstm_output = decoder_lstm(decoder_embedding[:, i, :], initial_state=encoder_states)\n",
        "#     decoder_dense = TimeDistributed(Dense(max_words, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_lstm_output))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ## model 2\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # Decoder\n",
        "# # decoder_inputs = Input(shape=(12, max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_inputs = Input(shape=(12, None,), name='decoder_inputs')\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "#                                               trainable=False))(decoder_inputs)\n",
        "\n",
        "# # Process each target sequence individually\n",
        "# decoder_outputs = []\n",
        "# for i in range(12):\n",
        "#     decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=False, name=f'decoder_lstm_{i}')\n",
        "#     decoder_lstm_output = decoder_lstm(decoder_embedding[:, i, :], initial_state=encoder_states)\n",
        "#     decoder_dense = TimeDistributed(Dense(max_words, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_lstm_output))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### model with bidirectional layer .. good accuracy\n",
        "# from keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, TimeDistributed, Lambda, Concatenate\n",
        "# from keras.models import Model\n",
        "# import keras.backend as K\n",
        "\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True, name='encoder_lstm', dropout=0.5, recurrent_dropout=0.5))\n",
        "# encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "# # Concatenate forward and backward states\n",
        "# state_h = Concatenate()([forward_h, backward_h])\n",
        "# state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                                               trainable=False))(decoder_inputs)\n",
        "\n",
        "# decoder_outputs = []\n",
        "# for i in range(31):\n",
        "#     decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_{i}',\n",
        "#                         dropout=0.5, recurrent_dropout=0.5)\n",
        "#     decoder_lstm_output = decoder_lstm(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#     decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_lstm_output))\n",
        "\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#           ### multiple Bidirectional and LSTM\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True, name='encoder_lstm_1', dropout=0.5, recurrent_dropout=0.5))(encoder_embedding)\n",
        "# encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "#                                                         name='encoder_lstm_2', dropout=0.5, recurrent_dropout=0.5))(encoder_lstm1)\n",
        "# state_h = Concatenate()([forward_h2, backward_h2])\n",
        "# state_c = Concatenate()([forward_c2, backward_c2])\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                                               trainable=False))(decoder_inputs)\n",
        "# decoder_outputs = []\n",
        "# for i in range(31):\n",
        "#     decoder_lstm_1 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
        "#                         dropout=0.5, recurrent_dropout=0.5)\n",
        "#     decoder_lstm_2 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
        "#                         dropout=0.5, recurrent_dropout=0.5)\n",
        "#     decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#     decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "#     decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              #### multiple Bidirectional and LSTM and regularization\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                               input_length=max_sequence_length, trainable=False,\n",
        "#                               embeddings_regularizer=l2(1e-4))(encoder_inputs)\n",
        "# encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True, name='encoder_lstm_1', dropout=0.5, recurrent_dropout=0.5,\n",
        "#                                    kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4)))(encoder_embedding)\n",
        "# encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "#                                                         name='encoder_lstm_2', dropout=0.5, recurrent_dropout=0.5,\n",
        "#                                                         kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4)))(encoder_lstm1)\n",
        "# state_h = Concatenate()([forward_h2, backward_h2])\n",
        "# state_c = Concatenate()([forward_c2, backward_c2])\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                                               trainable=False, embeddings_regularizer=l2(1e-4)))(decoder_inputs)\n",
        "# decoder_outputs = []\n",
        "# for i in range(31):\n",
        "#     decoder_lstm_1 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
        "#                           dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4))\n",
        "#     decoder_lstm_2 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
        "#                           dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4))\n",
        "#     decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#     decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "#     decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax', kernel_regularizer=l2(1e-4)), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      ### GRU model\n",
        "# from keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, TimeDistributed, Lambda, Concatenate, GRU\n",
        "# from keras.models import Model\n",
        "# import keras.backend as K\n",
        "\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# encoder_gru = Bidirectional(GRU(latent_dim, return_state=True, name='encoder_gru', dropout=0.5, recurrent_dropout=0.5))\n",
        "# encoder_outputs, forward_h, backward_h = encoder_gru(encoder_embedding)\n",
        "\n",
        "# # Concatenate forward and backward states\n",
        "# state_h = Concatenate()([forward_h, backward_h])\n",
        "\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')  # +1 for the start token\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                                               trainable=False))(decoder_inputs)\n",
        "\n",
        "# decoder_outputs = []\n",
        "# for i in range(31):\n",
        "#     decoder_gru = GRU(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_gru_{i}', dropout=0.5, recurrent_dropout=0.5)\n",
        "#     decoder_gru_output = decoder_gru(decoder_embedding[:, i, :], initial_state=[state_h])\n",
        "#     decoder_dense = TimeDistributed(Dense(vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_gru_output))\n",
        "\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #### model 5\n",
        "# from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "### Encoder\n",
        "# encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "# encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                               input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "# transformer_layer = MultiHeadAttention(num_heads=4, key_dim=latent_dim)\n",
        "# transformer_output = transformer_layer(encoder_embedding, encoder_embedding, encoder_embedding)\n",
        "# transformer_output = LayerNormalization()(transformer_output + encoder_embedding)\n",
        "#### Decoder\n",
        "# decoder_inputs = Input(shape=(31, y_max_sequence_length,), name='decoder_inputs')\n",
        "# decoder_embedding = TimeDistributed(Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "#                                               trainable=False))(decoder_inputs)\n",
        "# decoder_outputs = []\n",
        "# for i in range(31):\n",
        "#     decoder_transformer_output = transformer_layer(decoder_embedding[:, i, :], transformer_output, transformer_output)\n",
        "#     decoder_transformer_output = LayerNormalization()(decoder_transformer_output + decoder_embedding[:, i, :])\n",
        "#     decoder_dense = TimeDistributed(Dense(vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#     decoder_outputs.append(decoder_dense(decoder_transformer_output))\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXq_Yyp98qiT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DzIgAMag4Ta"
      },
      "outputs": [],
      "source": [
        "# print(\"encoder input shape  => \", encoder_inputs.shape)\n",
        "# print(\"encoder embedding shape  => \", encoder_embedding.shape)\n",
        "# # print(\"encoder shape  => \", encoder_outputs.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
        "\n",
        "# print(\"encoder shape  => \", forward_h2.shape, forward_c2.shape, backward_h2.shape, backward_c2.shape)\n",
        "# print(\"encoder output shape  => \", encoder_lstm2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FKUbdXS6oXK"
      },
      "outputs": [],
      "source": [
        "# print(\"state_h  \", state_h.shape)\n",
        "# print(\"state_c  \", state_c.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_jjZAurhEtv"
      },
      "outputs": [],
      "source": [
        "# print(\"decoder input shape  => \", decoder_inputs.shape)\n",
        "# print(\"decoder embedding shape  => \", decoder_embedding.shape)\n",
        "# print(\"decoder lstm out shape  => \", decoder_lstm_output.shape)\n",
        "# print(\"decoder dense  => \", decoder_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBb2FuZ-7DRe"
      },
      "outputs": [],
      "source": [
        "# print(\"decoder dense  => \", decoder_dense.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4NN_4KgkM0bo"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDfhfAdtAd1W"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1JKzd2LAh4t"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEaeFsZJ7WlI"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Classp_G7FDo"
      },
      "outputs": [],
      "source": [
        "# from keras_tuner import HyperModel\n",
        "# import keras_tuner as kt\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.regularizers import l1_l2, l2\n",
        "\n",
        "# class NLPHyperModel(HyperModel):\n",
        "#     def __init__(self, vocab_size_input, embedding_dim, max_sequence_length, embedding_matrix_input, y_vocab_size_input, y_max_sequence_length):\n",
        "#         self.vocab_size_input = vocab_size_input\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding_matrix_input = embedding_matrix_input\n",
        "#         self.y_vocab_size_input = y_vocab_size_input\n",
        "#         self.y_max_sequence_length = y_max_sequence_length\n",
        "\n",
        "#     def build(self, hp):\n",
        "#         latent_dim = hp.Int('latent_dim', min_value=32, max_value=256, step=32)\n",
        "#         dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)\n",
        "#         recurrent_dropout_rate = hp.Float('recurrent_dropout_rate', min_value=0.3, max_value=0.7, step=0.1)\n",
        "#         learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
        "#         # learning_rate = hp.Float('learning_rate', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "#         l1_reg = hp.Float('l1_reg', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
        "#         l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
        "#         ### l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "#         ### l2(1e-4)\n",
        "\n",
        "\n",
        "#         # Encoder\n",
        "#         encoder_inputs = Input(shape=(self.max_sequence_length,), name='encoder_inputs')\n",
        "#         encoder_embedding = Embedding(input_dim=self.vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                       input_length=self.max_sequence_length, trainable=False)(encoder_inputs)\n",
        "#         encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True, name='encoder_lstm_1', dropout=dropout_rate,\n",
        "#                                            recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4)))(encoder_embedding)\n",
        "#         encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "#                                                             name='encoder_lstm_2', dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate,\n",
        "#                                                                                              kernel_regularizer=l2(1e-4)))(encoder_lstm1)\n",
        "\n",
        "#         state_h = Concatenate()([forward_h2, backward_h2])\n",
        "#         state_c = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "#         # Decoder\n",
        "#         decoder_inputs = Input(shape=(7, y_max_sequence_length,), name='decoder_inputs')\n",
        "#         decoder_embedding = TimeDistributed(Embedding(input_dim=self.y_vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                                       trainable=False))(decoder_inputs)\n",
        "\n",
        "#         decoder_outputs = []\n",
        "#         for i in range(7):\n",
        "#             decoder_lstm_1 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
        "#                                   dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4))\n",
        "#             decoder_lstm_2 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
        "#                                   dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4))\n",
        "#             decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#             decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "#             decoder_dense = TimeDistributed(Dense(self.y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "#             decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "\n",
        "#         optimizer = Adam(learning_rate=learning_rate)\n",
        "#         model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#         # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#         model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#         return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SmgHkm2XS_f"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Concatenate, Dense, TimeDistributed\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.regularizers import l1_l2\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from kerastuner import HyperModel, RandomSearch\n",
        "# from keras_tuner import HyperModel\n",
        "# import keras_tuner as kt\n",
        "\n",
        "\n",
        "# class NLPHyperModel(HyperModel):\n",
        "#     def __init__(self, vocab_size_input, embedding_dim, max_sequence_length, embedding_matrix_input, y_vocab_size_input, y_max_sequence_length):\n",
        "#         self.vocab_size_input = vocab_size_input\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding_matrix_input = embedding_matrix_input\n",
        "#         self.y_vocab_size_input = y_vocab_size_input\n",
        "#         self.y_max_sequence_length = y_max_sequence_length\n",
        "\n",
        "#     def build(self, hp):\n",
        "#         latent_dim = hp.Int('latent_dim', min_value=32, max_value=256, step=32)\n",
        "#         dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n",
        "#         recurrent_dropout_rate = hp.Float('recurrent_dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n",
        "#         learning_rate = hp.Float('learning_rate', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "#         # l1_reg = hp.Float('l1_reg', min_value=1e-6, max_value=1e-2, sampling='LOG')\n",
        "#         # l2_reg = hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='LOG')\n",
        "#         l1_reg = hp.Float('l1_reg', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "#         l2_reg = hp.Float('l2_reg', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "\n",
        "#         # Encoder\n",
        "#         encoder_inputs = Input(shape=(self.max_sequence_length,), name='encoder_inputs')\n",
        "#         encoder_embedding = Embedding(input_dim=self.vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                       input_length=self.max_sequence_length, trainable=False, embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))(encoder_inputs)\n",
        "#         encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True, name='encoder_lstm_1', dropout=dropout_rate,\n",
        "#                                            recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))(encoder_embedding)\n",
        "#         encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "#                                                             name='encoder_lstm_2', dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))(encoder_lstm1)\n",
        "\n",
        "#         state_h = Concatenate()([forward_h2, backward_h2])\n",
        "#         state_c = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "#         # Decoder\n",
        "#         decoder_inputs = Input(shape=(31, self.y_max_sequence_length,), name='decoder_inputs')\n",
        "#         decoder_embedding = TimeDistributed(Embedding(input_dim=self.y_vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                                       trainable=False, embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))(decoder_inputs)\n",
        "\n",
        "#         decoder_outputs = []\n",
        "#         for i in range(31):\n",
        "#             decoder_lstm_1 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
        "#                                   dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))\n",
        "#             decoder_lstm_2 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
        "#                                   dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))\n",
        "#             decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#             decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "#             decoder_dense = TimeDistributed(Dense(self.y_vocab_size_input, activation='softmax', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)), name=f'decoder_dense_{i}')\n",
        "#             decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "\n",
        "#         model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#         optimizer = Adam(learning_rate=learning_rate)\n",
        "#         model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#         return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFnWZLpqfyod"
      },
      "outputs": [],
      "source": [
        "      ##### model with transformer encoder decoder\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout, TimeDistributed, Add\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from tensorflow.keras.regularizers import l1_l2\n",
        "# from tensorflow.keras.layers import Layer\n",
        "# from keras_tuner import HyperModel, RandomSearch\n",
        "\n",
        "# class TransformerBlock(Layer):\n",
        "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "#         self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "#         self.ffn = tf.keras.Sequential(\n",
        "#             [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "#         )\n",
        "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout1 = Dropout(rate)\n",
        "#         self.dropout2 = Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs, training):\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=training)\n",
        "#         out1 = self.layernorm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
        "#         return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# class NLPHyperModel(HyperModel):\n",
        "#     def __init__(self, vocab_size_input, embedding_dim, max_sequence_length, embedding_matrix_input, y_vocab_size_input, y_max_sequence_length):\n",
        "#         self.vocab_size_input = vocab_size_input\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding_matrix_input = embedding_matrix_input\n",
        "#         self.y_vocab_size_input = y_vocab_size_input\n",
        "#         self.y_max_sequence_length = y_max_sequence_length\n",
        "\n",
        "#     def build(self, hp):\n",
        "#         embed_dim = hp.Int('embed_dim', min_value=32, max_value=256, step=32)\n",
        "#         num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "#         ff_dim = hp.Int('ff_dim', min_value=32, max_value=256, step=32)\n",
        "#         dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "#         learning_rate = hp.Float('learning_rate', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "#         # l1_reg = hp.Float('l1_reg', min_value=1e-6, max_value=1e-2, sampling='LOG')\n",
        "#         # l2_reg = hp.Float('l2_reg', min_value=1e-6, max_value=1e-2, sampling='LOG')\n",
        "#         l1_reg = hp.Float('l1_reg', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "#         l2_reg = hp.Float('l2_reg', min_value=0.001, max_value=0.01, sampling='LOG')\n",
        "\n",
        "\n",
        "#         # Encoder\n",
        "#         encoder_inputs = Input(shape=(self.max_sequence_length,), name='encoder_inputs')\n",
        "#         encoder_embedding = Embedding(input_dim=self.vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                       input_length=self.max_sequence_length, trainable=False, embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))(encoder_inputs)\n",
        "#         encoder_transformer_block = TransformerBlock(self.embedding_dim, num_heads, ff_dim, dropout_rate)\n",
        "#         encoder_outputs = encoder_transformer_block(encoder_embedding)\n",
        "\n",
        "#         # Decoder\n",
        "#         decoder_inputs = Input(shape=(31, self.y_max_sequence_length,), name='decoder_inputs')\n",
        "#         decoder_embedding = TimeDistributed(Embedding(input_dim=self.y_vocab_size_input, output_dim=self.embedding_dim, weights=[self.embedding_matrix_input],\n",
        "#                                                       trainable=False, embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))(decoder_inputs)\n",
        "#         decoder_transformer_blocks = [TransformerBlock(self.embedding_dim, num_heads, ff_dim, dropout_rate) for _ in range(31)]\n",
        "#         decoder_outputs = [decoder_transformer_blocks[i](decoder_embedding[:, i, :]) for i in range(31)]\n",
        "#         decoder_dense = TimeDistributed(Dense(self.y_vocab_size_input, activation='softmax', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "#         decoder_outputs = [decoder_dense(output) for output in decoder_outputs]\n",
        "\n",
        "#         model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#         optimizer = Adam(learning_rate=learning_rate)\n",
        "#         model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#         return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCSeA-14_W5b"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Embedding, LSTM, Bidirectional, Concatenate, Dense, Input, TimeDistributed, LayerNormalization, Dropout\n",
        "# from keras.models import Model\n",
        "# from keras.regularizers import l1_l2\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# import keras_tuner as kt\n",
        "# from keras_tuner import HyperModel\n",
        "# from tensorflow.keras.layers import Layer\n",
        "# import tensorflow as tf\n",
        "\n",
        "# class TransformerBlock(Layer):\n",
        "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "#         self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "#         self.ffn = tf.keras.Sequential(\n",
        "#             [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "#         )\n",
        "#         self.layernorm1 = LayerNormalization(epsilon=1e-5)\n",
        "#         self.layernorm2 = LayerNormalization(epsilon=1e-5)\n",
        "#         self.dropout1 = Dropout(rate)\n",
        "#         self.dropout2 = Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs, training):\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=training)\n",
        "#         out1 = self.layernorm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
        "#         return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# class NLPHyperModel(HyperModel):\n",
        "#     def __init__(self, vocab_size_input, embedding_dim, max_sequence_length, embedding_matrix_input, y_vocab_size_input, y_max_sequence_length):\n",
        "#         self.vocab_size_input = vocab_size_input\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding_matrix_input = embedding_matrix_input\n",
        "#         self.y_vocab_size_input = y_vocab_size_input\n",
        "#         self.y_max_sequence_length = y_max_sequence_length\n",
        "\n",
        "#     def build(self, hp):\n",
        "#         latent_dim = hp.Int('latent_dim', min_value=32, max_value=128, step=32)\n",
        "#         dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n",
        "#         recurrent_dropout_rate = hp.Float('recurrent_dropout_rate', min_value=0.2, max_value=0.7, step=0.1)\n",
        "#         num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "#         ff_dim = hp.Int('ff_dim', min_value=32, max_value=256, step=32)\n",
        "#         l1_reg = hp.Float('l1', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
        "#         l2_reg = hp.Float('l2', min_value=1e-5, max_value=1e-2, sampling='LOG')\n",
        "#         learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "#         # learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG')\n",
        "#         # l1_reg = hp.Float('l1_reg', min_value=0.0001, max_value=0.01, sampling='LOG')\n",
        "#         # l2_reg = hp.Float('l2_reg', min_value=0.0001, max_value=0.01, sampling='LOG')\n",
        "\n",
        "\n",
        "#         # Encoder\n",
        "#         encoder_inputs = Input(shape=(self.max_sequence_length,), name='encoder_inputs')\n",
        "#         encoder_embedding = Embedding(\n",
        "#             input_dim=self.vocab_size_input,\n",
        "#             output_dim=self.embedding_dim,\n",
        "#             weights=[self.embedding_matrix_input],\n",
        "#             input_length=self.max_sequence_length,\n",
        "#             trainable=False,\n",
        "#             # embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "#             embeddings_regularizer=l2(1e-4)\n",
        "#         )(encoder_inputs)\n",
        "\n",
        "#         encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True, name='encoder_lstm_1',\n",
        "#                                            dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4),\n",
        "#                                            recurrent_regularizer=l2(1e-4)))(encoder_embedding)\n",
        "#         encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(latent_dim, return_sequences=True,\n",
        "#                                                                                              return_state=True, name='encoder_lstm_2',\n",
        "#                                                                                              dropout=dropout_rate,\n",
        "#                                                                                              recurrent_dropout=recurrent_dropout_rate,\n",
        "#                                                                                              kernel_regularizer=l2(1e-4),\n",
        "#                                                                                              recurrent_regularizer=l2(1e-4)))(encoder_lstm1)\n",
        "\n",
        "#         encoder_transformer_block = TransformerBlock(latent_dim * 2, num_heads, ff_dim, dropout_rate)\n",
        "#         encoder_outputs = encoder_transformer_block(encoder_lstm2)\n",
        "\n",
        "#         state_h = Concatenate()([forward_h2, backward_h2])  # forward_h, backward_h\n",
        "#         state_c = Concatenate()([forward_c2, backward_c2])  # forward_c, backward_c\n",
        "\n",
        "#         # Decoder\n",
        "#         decoder_inputs = Input(shape=(7, self.y_max_sequence_length,), name='decoder_inputs')\n",
        "#         decoder_embedding = TimeDistributed(Embedding(\n",
        "#             input_dim=self.y_vocab_size_input,\n",
        "#             output_dim=self.embedding_dim,\n",
        "#             weights=[self.embedding_matrix_input],\n",
        "#             trainable=False,\n",
        "#             # embeddings_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)\n",
        "#             embeddings_regularizer=l2(1e-4)\n",
        "#         ))(decoder_inputs)\n",
        "\n",
        "#         decoder_outputs = []\n",
        "#         for i in range(7):\n",
        "#             decoder_lstm_1 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}', dropout=dropout_rate,\n",
        "#                                   recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4))\n",
        "#             decoder_lstm_2 = LSTM(latent_dim * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}', dropout=dropout_rate,\n",
        "#                                   recurrent_dropout=recurrent_dropout_rate, kernel_regularizer=l2(1e-4), recurrent_regularizer=l2(1e-4))\n",
        "#             decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "#             decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "#             # decoder_dense = TimeDistributed(Dense(self.y_vocab_size_input, activation='softmax', kernel_regularizer=l2(1e-4)),\n",
        "#             #                                 name=f'decoder_dense_{i}')\n",
        "#             decoder_dense = TimeDistributed(Dense(self.y_vocab_size_input, activation='softmax', kernel_regularizer=l2(1e-4)),\n",
        "#                                             name=f'decoder_dense_{i}')\n",
        "#             decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "\n",
        "#         model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#         optimizer = Adam(learning_rate=learning_rate)\n",
        "#         model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#         return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYdGOYiA7PTg"
      },
      "outputs": [],
      "source": [
        "# from keras_tuner import RandomSearch\n",
        "\n",
        "# # Define the hypermodel\n",
        "# hypermodel = NLPHyperModel(\n",
        "#     vocab_size_input=vocab_size_input,\n",
        "#     embedding_dim=embedding_dim,\n",
        "#     max_sequence_length=max_sequence_length,\n",
        "#     embedding_matrix_input=embedding_matrix_input,\n",
        "#     y_vocab_size_input=y_vocab_size_input,\n",
        "#     y_max_sequence_length= y_max_sequence_length\n",
        "# )\n",
        "\n",
        "# # Initialize the tuner\n",
        "# tuner = RandomSearch(\n",
        "#     hypermodel,\n",
        "#     objective='val_loss',\n",
        "#     max_trials=1,   ## number of model it will search\n",
        "#     executions_per_trial=1,   ## numberof time each model will train\n",
        "#     directory='my_dir',\n",
        "#     project_name='nlp_tuning'\n",
        "# )\n",
        "\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# # Define the EarlyStopping callback\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, cooldown=0)\n",
        "\n",
        "# # Run the tuner search\n",
        "# # tuner.search([X_train, y_train], [y_train[:, i, :] for i in range(y_train.shape[1])], epochs=50,\n",
        "# #              validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
        "# tuner.search([X_train, y_train], [y_train[:, i, :] for i in range(y_train.shape[1])], epochs=30, batch_size= 64,\n",
        "#              validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z6TzxwA7e1G"
      },
      "outputs": [],
      "source": [
        "# best_model = tuner.get_best_models(num_models=1)[0]\n",
        "# best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bFDK6P58sVT"
      },
      "outputs": [],
      "source": [
        "# # Define the EarlyStopping callback\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, cooldown=0)\n",
        "\n",
        "# # Train the best model further if needed\n",
        "# history = best_model.fit([X_train, y_train], [y_train[:, i, :] for i in range(y_train.shape[1])], epochs=50, validation_split=0.2, batch_size= 64,\n",
        "#                          callbacks=[reduce_lr, early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import Attention\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "                              input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "encoder_lstm1 = Bidirectional(LSTM(96, return_sequences=True, name='encoder_lstm_1', dropout=0.3,\n",
        "                                    recurrent_dropout=0.3, kernel_regularizer=l2(1e-4)))(encoder_embedding)\n",
        "encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(96, return_sequences=True, return_state=True,\n",
        "                                                    name='encoder_lstm_2', dropout=0.3, recurrent_dropout=0.3,\n",
        "                                                    kernel_regularizer=l2(1e-4)))(encoder_lstm1)\n",
        "\n",
        "state_h = Concatenate()([forward_h2, backward_h2])\n",
        "state_c = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(21, y_max_sequence_length,), name='decoder_inputs')\n",
        "decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "                                              trainable=False))(decoder_inputs)\n",
        "\n",
        "attention = Attention()\n",
        "\n",
        "decoder_outputs = []\n",
        "for i in range(21):\n",
        "    decoder_lstm_1 = LSTM(96 * 2, return_sequences=True, return_state=True, name=f'decoder_lstm_1_{i}',\n",
        "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
        "    decoder_lstm_2 = LSTM(96 * 2, return_sequences=True, return_state=True, name=f'decoder_lstm_2_{i}',\n",
        "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
        "    \n",
        "    lstm_1_output, _, _ = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "    attention_output = attention([lstm_1_output, encoder_lstm2])\n",
        "    decoder_lstm_output_2, _, _ = decoder_lstm_2(attention_output, initial_state=[state_h, state_c])\n",
        "    \n",
        "    decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "    decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.004872)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, cooldown=0)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ftd0lGy6MfN"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "encoder_inputs = Input(shape=(max_sequence_length,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "                              input_length=max_sequence_length, trainable=False)(encoder_inputs)\n",
        "encoder_lstm1 = Bidirectional(LSTM(96, return_sequences=True, name='encoder_lstm_1', dropout=0.3,\n",
        "                                    recurrent_dropout=0.3, kernel_regularizer=l2(1e-4)))(encoder_embedding)\n",
        "encoder_lstm2, forward_h2, forward_c2, backward_h2, backward_c2 = Bidirectional(LSTM(96, return_sequences=True, return_state=True,\n",
        "                                                    name='encoder_lstm_2', dropout=0.3, recurrent_dropout=0.3,\n",
        "                                                                                      kernel_regularizer=l2(1e-4)))(encoder_lstm1)\n",
        "\n",
        "state_h = Concatenate()([forward_h2, backward_h2])\n",
        "state_c = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "### best LSTM = 128\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(21, y_max_sequence_length,), name='decoder_inputs')\n",
        "decoder_embedding = TimeDistributed(Embedding(input_dim=y_vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input],\n",
        "                                              trainable=False))(decoder_inputs)\n",
        "\n",
        "decoder_outputs = []\n",
        "for i in range(21):\n",
        "    decoder_lstm_1 = LSTM(96 * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_1_{i}',\n",
        "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
        "    decoder_lstm_2 = LSTM(96 * 2, return_sequences=True, return_state=False, name=f'decoder_lstm_2_{i}',\n",
        "                          dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(1e-4))\n",
        "    decoder_lstm_output_1 = decoder_lstm_1(decoder_embedding[:, i, :], initial_state=[state_h, state_c])\n",
        "    decoder_lstm_output_2 = decoder_lstm_2(decoder_lstm_output_1, initial_state=[state_h, state_c])\n",
        "    decoder_dense = TimeDistributed(Dense(y_vocab_size_input, activation='softmax'), name=f'decoder_dense_{i}')\n",
        "    decoder_outputs.append(decoder_dense(decoder_lstm_output_2))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.004872)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5, cooldown=0)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIvMErWHEcsy",
        "outputId": "4f14225e-4d5e-4cba-f4ad-0c9d40fdec3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "40/40 [==============================] - 1304s 29s/step - loss: 18.9756 - decoder_dense_0_loss: 1.4956 - decoder_dense_1_loss: 0.7833 - decoder_dense_2_loss: 0.9041 - decoder_dense_3_loss: 0.8109 - decoder_dense_4_loss: 0.8152 - decoder_dense_5_loss: 0.7785 - decoder_dense_6_loss: 0.8966 - decoder_dense_7_loss: 0.7591 - decoder_dense_8_loss: 0.8156 - decoder_dense_9_loss: 0.8137 - decoder_dense_10_loss: 0.9090 - decoder_dense_11_loss: 0.8018 - decoder_dense_12_loss: 0.8021 - decoder_dense_13_loss: 0.8509 - decoder_dense_14_loss: 0.7795 - decoder_dense_15_loss: 0.8098 - decoder_dense_16_loss: 0.8059 - decoder_dense_17_loss: 0.7751 - decoder_dense_18_loss: 0.8275 - decoder_dense_19_loss: 1.0161 - decoder_dense_20_loss: 0.9051 - decoder_dense_0_accuracy: 0.8547 - decoder_dense_1_accuracy: 0.9643 - decoder_dense_2_accuracy: 0.9276 - decoder_dense_3_accuracy: 0.9503 - decoder_dense_4_accuracy: 0.9566 - decoder_dense_5_accuracy: 0.9631 - decoder_dense_6_accuracy: 0.9317 - decoder_dense_7_accuracy: 0.9599 - decoder_dense_8_accuracy: 0.9456 - decoder_dense_9_accuracy: 0.9584 - decoder_dense_10_accuracy: 0.9334 - decoder_dense_11_accuracy: 0.9463 - decoder_dense_12_accuracy: 0.9543 - decoder_dense_13_accuracy: 0.9527 - decoder_dense_14_accuracy: 0.9682 - decoder_dense_15_accuracy: 0.9671 - decoder_dense_16_accuracy: 0.9507 - decoder_dense_17_accuracy: 0.9516 - decoder_dense_18_accuracy: 0.9575 - decoder_dense_19_accuracy: 0.9167 - decoder_dense_20_accuracy: 0.9274 - val_loss: 2.7441 - val_decoder_dense_0_loss: 0.7532 - val_decoder_dense_1_loss: 0.0144 - val_decoder_dense_2_loss: 0.1372 - val_decoder_dense_3_loss: 0.0461 - val_decoder_dense_4_loss: 0.0318 - val_decoder_dense_5_loss: 0.0126 - val_decoder_dense_6_loss: 0.0988 - val_decoder_dense_7_loss: 0.0239 - val_decoder_dense_8_loss: 0.0594 - val_decoder_dense_9_loss: 0.0347 - val_decoder_dense_10_loss: 0.1176 - val_decoder_dense_11_loss: 0.0580 - val_decoder_dense_12_loss: 0.0332 - val_decoder_dense_13_loss: 0.0714 - val_decoder_dense_14_loss: 0.0106 - val_decoder_dense_15_loss: 0.0232 - val_decoder_dense_16_loss: 0.0455 - val_decoder_dense_17_loss: 0.0374 - val_decoder_dense_18_loss: 0.0244 - val_decoder_dense_19_loss: 0.2127 - val_decoder_dense_20_loss: 0.1371 - val_decoder_dense_0_accuracy: 0.8834 - val_decoder_dense_1_accuracy: 0.9979 - val_decoder_dense_2_accuracy: 0.9595 - val_decoder_dense_3_accuracy: 0.9797 - val_decoder_dense_4_accuracy: 0.9872 - val_decoder_dense_5_accuracy: 0.9991 - val_decoder_dense_6_accuracy: 0.9608 - val_decoder_dense_7_accuracy: 0.9872 - val_decoder_dense_8_accuracy: 0.9732 - val_decoder_dense_9_accuracy: 0.9903 - val_decoder_dense_10_accuracy: 0.9621 - val_decoder_dense_11_accuracy: 0.9793 - val_decoder_dense_12_accuracy: 0.9960 - val_decoder_dense_13_accuracy: 0.9875 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9792 - val_decoder_dense_17_accuracy: 0.9805 - val_decoder_dense_18_accuracy: 0.9857 - val_decoder_dense_19_accuracy: 0.9564 - val_decoder_dense_20_accuracy: 0.9555 - lr: 0.0049\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 1168s 29s/step - loss: 2.3353 - decoder_dense_0_loss: 0.6912 - decoder_dense_1_loss: 0.0106 - decoder_dense_2_loss: 0.1222 - decoder_dense_3_loss: 0.0379 - decoder_dense_4_loss: 0.0287 - decoder_dense_5_loss: 0.0068 - decoder_dense_6_loss: 0.0725 - decoder_dense_7_loss: 0.0195 - decoder_dense_8_loss: 0.0451 - decoder_dense_9_loss: 0.0278 - decoder_dense_10_loss: 0.0922 - decoder_dense_11_loss: 0.0445 - decoder_dense_12_loss: 0.0266 - decoder_dense_13_loss: 0.0578 - decoder_dense_14_loss: 0.0083 - decoder_dense_15_loss: 0.0173 - decoder_dense_16_loss: 0.0336 - decoder_dense_17_loss: 0.0285 - decoder_dense_18_loss: 0.0184 - decoder_dense_19_loss: 0.1968 - decoder_dense_20_loss: 0.1194 - decoder_dense_0_accuracy: 0.8858 - decoder_dense_1_accuracy: 0.9981 - decoder_dense_2_accuracy: 0.9618 - decoder_dense_3_accuracy: 0.9836 - decoder_dense_4_accuracy: 0.9877 - decoder_dense_5_accuracy: 0.9994 - decoder_dense_6_accuracy: 0.9736 - decoder_dense_7_accuracy: 0.9911 - decoder_dense_8_accuracy: 0.9811 - decoder_dense_9_accuracy: 0.9899 - decoder_dense_10_accuracy: 0.9662 - decoder_dense_11_accuracy: 0.9801 - decoder_dense_12_accuracy: 0.9938 - decoder_dense_13_accuracy: 0.9869 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9854 - decoder_dense_17_accuracy: 0.9881 - decoder_dense_18_accuracy: 0.9922 - decoder_dense_19_accuracy: 0.9572 - decoder_dense_20_accuracy: 0.9608 - val_loss: 2.0111 - val_decoder_dense_0_loss: 0.7085 - val_decoder_dense_1_loss: 0.0098 - val_decoder_dense_2_loss: 0.1030 - val_decoder_dense_3_loss: 0.0299 - val_decoder_dense_4_loss: 0.0246 - val_decoder_dense_5_loss: 0.0056 - val_decoder_dense_6_loss: 0.0365 - val_decoder_dense_7_loss: 0.0099 - val_decoder_dense_8_loss: 0.0279 - val_decoder_dense_9_loss: 0.0192 - val_decoder_dense_10_loss: 0.0610 - val_decoder_dense_11_loss: 0.0271 - val_decoder_dense_12_loss: 0.0194 - val_decoder_dense_13_loss: 0.0597 - val_decoder_dense_14_loss: 0.0089 - val_decoder_dense_15_loss: 0.0194 - val_decoder_dense_16_loss: 0.0489 - val_decoder_dense_17_loss: 0.0146 - val_decoder_dense_18_loss: 0.0087 - val_decoder_dense_19_loss: 0.1738 - val_decoder_dense_20_loss: 0.0882 - val_decoder_dense_0_accuracy: 0.8857 - val_decoder_dense_1_accuracy: 0.9977 - val_decoder_dense_2_accuracy: 0.9709 - val_decoder_dense_3_accuracy: 0.9897 - val_decoder_dense_4_accuracy: 0.9911 - val_decoder_dense_5_accuracy: 0.9991 - val_decoder_dense_6_accuracy: 0.9966 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9958 - val_decoder_dense_9_accuracy: 0.9947 - val_decoder_dense_10_accuracy: 0.9840 - val_decoder_dense_11_accuracy: 0.9967 - val_decoder_dense_12_accuracy: 0.9952 - val_decoder_dense_13_accuracy: 0.9871 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9941 - val_decoder_dense_17_accuracy: 0.9976 - val_decoder_dense_18_accuracy: 0.9989 - val_decoder_dense_19_accuracy: 0.9580 - val_decoder_dense_20_accuracy: 0.9808 - lr: 0.0049\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 1166s 29s/step - loss: 1.7822 - decoder_dense_0_loss: 0.6718 - decoder_dense_1_loss: 0.0088 - decoder_dense_2_loss: 0.1003 - decoder_dense_3_loss: 0.0291 - decoder_dense_4_loss: 0.0255 - decoder_dense_5_loss: 0.0038 - decoder_dense_6_loss: 0.0335 - decoder_dense_7_loss: 0.0081 - decoder_dense_8_loss: 0.0194 - decoder_dense_9_loss: 0.0174 - decoder_dense_10_loss: 0.0546 - decoder_dense_11_loss: 0.0226 - decoder_dense_12_loss: 0.0136 - decoder_dense_13_loss: 0.0459 - decoder_dense_14_loss: 0.0068 - decoder_dense_15_loss: 0.0151 - decoder_dense_16_loss: 0.0138 - decoder_dense_17_loss: 0.0129 - decoder_dense_18_loss: 0.0078 - decoder_dense_19_loss: 0.1601 - decoder_dense_20_loss: 0.0866 - decoder_dense_0_accuracy: 0.8869 - decoder_dense_1_accuracy: 0.9982 - decoder_dense_2_accuracy: 0.9679 - decoder_dense_3_accuracy: 0.9884 - decoder_dense_4_accuracy: 0.9896 - decoder_dense_5_accuracy: 0.9995 - decoder_dense_6_accuracy: 0.9944 - decoder_dense_7_accuracy: 0.9979 - decoder_dense_8_accuracy: 0.9963 - decoder_dense_9_accuracy: 0.9948 - decoder_dense_10_accuracy: 0.9834 - decoder_dense_11_accuracy: 0.9953 - decoder_dense_12_accuracy: 0.9970 - decoder_dense_13_accuracy: 0.9901 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9972 - decoder_dense_17_accuracy: 0.9969 - decoder_dense_18_accuracy: 0.9982 - decoder_dense_19_accuracy: 0.9597 - decoder_dense_20_accuracy: 0.9769 - val_loss: 1.6508 - val_decoder_dense_0_loss: 0.6969 - val_decoder_dense_1_loss: 0.0094 - val_decoder_dense_2_loss: 0.0887 - val_decoder_dense_3_loss: 0.0253 - val_decoder_dense_4_loss: 0.0238 - val_decoder_dense_5_loss: 0.0039 - val_decoder_dense_6_loss: 0.0232 - val_decoder_dense_7_loss: 0.0083 - val_decoder_dense_8_loss: 0.0197 - val_decoder_dense_9_loss: 0.0165 - val_decoder_dense_10_loss: 0.0484 - val_decoder_dense_11_loss: 0.0182 - val_decoder_dense_12_loss: 0.0082 - val_decoder_dense_13_loss: 0.0493 - val_decoder_dense_14_loss: 0.0079 - val_decoder_dense_15_loss: 0.0174 - val_decoder_dense_16_loss: 0.0112 - val_decoder_dense_17_loss: 0.0081 - val_decoder_dense_18_loss: 0.0057 - val_decoder_dense_19_loss: 0.1419 - val_decoder_dense_20_loss: 0.0667 - val_decoder_dense_0_accuracy: 0.8867 - val_decoder_dense_1_accuracy: 0.9979 - val_decoder_dense_2_accuracy: 0.9752 - val_decoder_dense_3_accuracy: 0.9895 - val_decoder_dense_4_accuracy: 0.9903 - val_decoder_dense_5_accuracy: 0.9994 - val_decoder_dense_6_accuracy: 0.9974 - val_decoder_dense_7_accuracy: 0.9979 - val_decoder_dense_8_accuracy: 0.9960 - val_decoder_dense_9_accuracy: 0.9947 - val_decoder_dense_10_accuracy: 0.9818 - val_decoder_dense_11_accuracy: 0.9974 - val_decoder_dense_12_accuracy: 0.9988 - val_decoder_dense_13_accuracy: 0.9904 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9978 - val_decoder_dense_17_accuracy: 0.9983 - val_decoder_dense_18_accuracy: 0.9986 - val_decoder_dense_19_accuracy: 0.9614 - val_decoder_dense_20_accuracy: 0.9860 - lr: 0.0049\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 1169s 29s/step - loss: 1.5416 - decoder_dense_0_loss: 0.6512 - decoder_dense_1_loss: 0.0084 - decoder_dense_2_loss: 0.0903 - decoder_dense_3_loss: 0.0262 - decoder_dense_4_loss: 0.0241 - decoder_dense_5_loss: 0.0031 - decoder_dense_6_loss: 0.0240 - decoder_dense_7_loss: 0.0076 - decoder_dense_8_loss: 0.0134 - decoder_dense_9_loss: 0.0163 - decoder_dense_10_loss: 0.0464 - decoder_dense_11_loss: 0.0179 - decoder_dense_12_loss: 0.0080 - decoder_dense_13_loss: 0.0387 - decoder_dense_14_loss: 0.0065 - decoder_dense_15_loss: 0.0144 - decoder_dense_16_loss: 0.0108 - decoder_dense_17_loss: 0.0107 - decoder_dense_18_loss: 0.0063 - decoder_dense_19_loss: 0.1388 - decoder_dense_20_loss: 0.0762 - decoder_dense_0_accuracy: 0.8873 - decoder_dense_1_accuracy: 0.9983 - decoder_dense_2_accuracy: 0.9715 - decoder_dense_3_accuracy: 0.9892 - decoder_dense_4_accuracy: 0.9901 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9961 - decoder_dense_7_accuracy: 0.9978 - decoder_dense_8_accuracy: 0.9977 - decoder_dense_9_accuracy: 0.9951 - decoder_dense_10_accuracy: 0.9860 - decoder_dense_11_accuracy: 0.9960 - decoder_dense_12_accuracy: 0.9985 - decoder_dense_13_accuracy: 0.9920 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9973 - decoder_dense_17_accuracy: 0.9972 - decoder_dense_18_accuracy: 0.9984 - decoder_dense_19_accuracy: 0.9626 - decoder_dense_20_accuracy: 0.9800 - val_loss: 1.4692 - val_decoder_dense_0_loss: 0.6710 - val_decoder_dense_1_loss: 0.0088 - val_decoder_dense_2_loss: 0.0824 - val_decoder_dense_3_loss: 0.0239 - val_decoder_dense_4_loss: 0.0222 - val_decoder_dense_5_loss: 0.0037 - val_decoder_dense_6_loss: 0.0151 - val_decoder_dense_7_loss: 0.0071 - val_decoder_dense_8_loss: 0.0176 - val_decoder_dense_9_loss: 0.0166 - val_decoder_dense_10_loss: 0.0423 - val_decoder_dense_11_loss: 0.0163 - val_decoder_dense_12_loss: 0.0046 - val_decoder_dense_13_loss: 0.0447 - val_decoder_dense_14_loss: 0.0071 - val_decoder_dense_15_loss: 0.0165 - val_decoder_dense_16_loss: 0.0111 - val_decoder_dense_17_loss: 0.0084 - val_decoder_dense_18_loss: 0.0049 - val_decoder_dense_19_loss: 0.1281 - val_decoder_dense_20_loss: 0.0594 - val_decoder_dense_0_accuracy: 0.8877 - val_decoder_dense_1_accuracy: 0.9979 - val_decoder_dense_2_accuracy: 0.9762 - val_decoder_dense_3_accuracy: 0.9904 - val_decoder_dense_4_accuracy: 0.9909 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9990 - val_decoder_dense_7_accuracy: 0.9981 - val_decoder_dense_8_accuracy: 0.9964 - val_decoder_dense_9_accuracy: 0.9946 - val_decoder_dense_10_accuracy: 0.9873 - val_decoder_dense_11_accuracy: 0.9975 - val_decoder_dense_12_accuracy: 0.9994 - val_decoder_dense_13_accuracy: 0.9911 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9977 - val_decoder_dense_17_accuracy: 0.9986 - val_decoder_dense_18_accuracy: 0.9991 - val_decoder_dense_19_accuracy: 0.9668 - val_decoder_dense_20_accuracy: 0.9871 - lr: 0.0049\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 1166s 29s/step - loss: 1.3855 - decoder_dense_0_loss: 0.6246 - decoder_dense_1_loss: 0.0080 - decoder_dense_2_loss: 0.0843 - decoder_dense_3_loss: 0.0242 - decoder_dense_4_loss: 0.0236 - decoder_dense_5_loss: 0.0028 - decoder_dense_6_loss: 0.0201 - decoder_dense_7_loss: 0.0066 - decoder_dense_8_loss: 0.0125 - decoder_dense_9_loss: 0.0156 - decoder_dense_10_loss: 0.0397 - decoder_dense_11_loss: 0.0149 - decoder_dense_12_loss: 0.0046 - decoder_dense_13_loss: 0.0343 - decoder_dense_14_loss: 0.0063 - decoder_dense_15_loss: 0.0142 - decoder_dense_16_loss: 0.0109 - decoder_dense_17_loss: 0.0096 - decoder_dense_18_loss: 0.0059 - decoder_dense_19_loss: 0.1263 - decoder_dense_20_loss: 0.0709 - decoder_dense_0_accuracy: 0.8889 - decoder_dense_1_accuracy: 0.9983 - decoder_dense_2_accuracy: 0.9729 - decoder_dense_3_accuracy: 0.9904 - decoder_dense_4_accuracy: 0.9903 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9967 - decoder_dense_7_accuracy: 0.9981 - decoder_dense_8_accuracy: 0.9975 - decoder_dense_9_accuracy: 0.9950 - decoder_dense_10_accuracy: 0.9880 - decoder_dense_11_accuracy: 0.9967 - decoder_dense_12_accuracy: 0.9992 - decoder_dense_13_accuracy: 0.9928 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9973 - decoder_dense_17_accuracy: 0.9973 - decoder_dense_18_accuracy: 0.9984 - decoder_dense_19_accuracy: 0.9654 - decoder_dense_20_accuracy: 0.9814 - val_loss: 1.3369 - val_decoder_dense_0_loss: 0.6464 - val_decoder_dense_1_loss: 0.0085 - val_decoder_dense_2_loss: 0.0789 - val_decoder_dense_3_loss: 0.0232 - val_decoder_dense_4_loss: 0.0222 - val_decoder_dense_5_loss: 0.0034 - val_decoder_dense_6_loss: 0.0118 - val_decoder_dense_7_loss: 0.0061 - val_decoder_dense_8_loss: 0.0168 - val_decoder_dense_9_loss: 0.0148 - val_decoder_dense_10_loss: 0.0308 - val_decoder_dense_11_loss: 0.0131 - val_decoder_dense_12_loss: 0.0034 - val_decoder_dense_13_loss: 0.0441 - val_decoder_dense_14_loss: 0.0072 - val_decoder_dense_15_loss: 0.0168 - val_decoder_dense_16_loss: 0.0102 - val_decoder_dense_17_loss: 0.0065 - val_decoder_dense_18_loss: 0.0042 - val_decoder_dense_19_loss: 0.1184 - val_decoder_dense_20_loss: 0.0531 - val_decoder_dense_0_accuracy: 0.8903 - val_decoder_dense_1_accuracy: 0.9979 - val_decoder_dense_2_accuracy: 0.9742 - val_decoder_dense_3_accuracy: 0.9902 - val_decoder_dense_4_accuracy: 0.9908 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9985 - val_decoder_dense_7_accuracy: 0.9984 - val_decoder_dense_8_accuracy: 0.9958 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9923 - val_decoder_dense_11_accuracy: 0.9980 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9912 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9976 - val_decoder_dense_17_accuracy: 0.9979 - val_decoder_dense_18_accuracy: 0.9990 - val_decoder_dense_19_accuracy: 0.9673 - val_decoder_dense_20_accuracy: 0.9907 - lr: 0.0049\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 1165s 29s/step - loss: 1.2378 - decoder_dense_0_loss: 0.5836 - decoder_dense_1_loss: 0.0075 - decoder_dense_2_loss: 0.0794 - decoder_dense_3_loss: 0.0232 - decoder_dense_4_loss: 0.0223 - decoder_dense_5_loss: 0.0027 - decoder_dense_6_loss: 0.0151 - decoder_dense_7_loss: 0.0055 - decoder_dense_8_loss: 0.0097 - decoder_dense_9_loss: 0.0140 - decoder_dense_10_loss: 0.0289 - decoder_dense_11_loss: 0.0124 - decoder_dense_12_loss: 0.0031 - decoder_dense_13_loss: 0.0316 - decoder_dense_14_loss: 0.0062 - decoder_dense_15_loss: 0.0140 - decoder_dense_16_loss: 0.0089 - decoder_dense_17_loss: 0.0084 - decoder_dense_18_loss: 0.0049 - decoder_dense_19_loss: 0.1182 - decoder_dense_20_loss: 0.0633 - decoder_dense_0_accuracy: 0.8912 - decoder_dense_1_accuracy: 0.9983 - decoder_dense_2_accuracy: 0.9735 - decoder_dense_3_accuracy: 0.9904 - decoder_dense_4_accuracy: 0.9910 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9975 - decoder_dense_7_accuracy: 0.9983 - decoder_dense_8_accuracy: 0.9981 - decoder_dense_9_accuracy: 0.9956 - decoder_dense_10_accuracy: 0.9925 - decoder_dense_11_accuracy: 0.9972 - decoder_dense_12_accuracy: 0.9995 - decoder_dense_13_accuracy: 0.9931 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9973 - decoder_dense_17_accuracy: 0.9976 - decoder_dense_18_accuracy: 0.9985 - decoder_dense_19_accuracy: 0.9663 - decoder_dense_20_accuracy: 0.9852 - val_loss: 1.2099 - val_decoder_dense_0_loss: 0.6008 - val_decoder_dense_1_loss: 0.0074 - val_decoder_dense_2_loss: 0.0707 - val_decoder_dense_3_loss: 0.0220 - val_decoder_dense_4_loss: 0.0214 - val_decoder_dense_5_loss: 0.0037 - val_decoder_dense_6_loss: 0.0097 - val_decoder_dense_7_loss: 0.0058 - val_decoder_dense_8_loss: 0.0162 - val_decoder_dense_9_loss: 0.0142 - val_decoder_dense_10_loss: 0.0211 - val_decoder_dense_11_loss: 0.0128 - val_decoder_dense_12_loss: 0.0028 - val_decoder_dense_13_loss: 0.0399 - val_decoder_dense_14_loss: 0.0078 - val_decoder_dense_15_loss: 0.0178 - val_decoder_dense_16_loss: 0.0086 - val_decoder_dense_17_loss: 0.0047 - val_decoder_dense_18_loss: 0.0031 - val_decoder_dense_19_loss: 0.1141 - val_decoder_dense_20_loss: 0.0503 - val_decoder_dense_0_accuracy: 0.8924 - val_decoder_dense_1_accuracy: 0.9981 - val_decoder_dense_2_accuracy: 0.9769 - val_decoder_dense_3_accuracy: 0.9906 - val_decoder_dense_4_accuracy: 0.9915 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9991 - val_decoder_dense_7_accuracy: 0.9981 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9949 - val_decoder_dense_10_accuracy: 0.9971 - val_decoder_dense_11_accuracy: 0.9968 - val_decoder_dense_12_accuracy: 0.9997 - val_decoder_dense_13_accuracy: 0.9912 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9989 - val_decoder_dense_18_accuracy: 0.9994 - val_decoder_dense_19_accuracy: 0.9673 - val_decoder_dense_20_accuracy: 0.9899 - lr: 0.0049\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 1149s 29s/step - loss: 1.1274 - decoder_dense_0_loss: 0.5470 - decoder_dense_1_loss: 0.0073 - decoder_dense_2_loss: 0.0731 - decoder_dense_3_loss: 0.0207 - decoder_dense_4_loss: 0.0209 - decoder_dense_5_loss: 0.0026 - decoder_dense_6_loss: 0.0136 - decoder_dense_7_loss: 0.0054 - decoder_dense_8_loss: 0.0088 - decoder_dense_9_loss: 0.0140 - decoder_dense_10_loss: 0.0234 - decoder_dense_11_loss: 0.0113 - decoder_dense_12_loss: 0.0027 - decoder_dense_13_loss: 0.0292 - decoder_dense_14_loss: 0.0062 - decoder_dense_15_loss: 0.0140 - decoder_dense_16_loss: 0.0075 - decoder_dense_17_loss: 0.0067 - decoder_dense_18_loss: 0.0041 - decoder_dense_19_loss: 0.1138 - decoder_dense_20_loss: 0.0550 - decoder_dense_0_accuracy: 0.8947 - decoder_dense_1_accuracy: 0.9983 - decoder_dense_2_accuracy: 0.9759 - decoder_dense_3_accuracy: 0.9915 - decoder_dense_4_accuracy: 0.9914 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9977 - decoder_dense_7_accuracy: 0.9983 - decoder_dense_8_accuracy: 0.9982 - decoder_dense_9_accuracy: 0.9952 - decoder_dense_10_accuracy: 0.9951 - decoder_dense_11_accuracy: 0.9971 - decoder_dense_12_accuracy: 0.9995 - decoder_dense_13_accuracy: 0.9933 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9964 - decoder_dense_16_accuracy: 0.9979 - decoder_dense_17_accuracy: 0.9981 - decoder_dense_18_accuracy: 0.9989 - decoder_dense_19_accuracy: 0.9673 - decoder_dense_20_accuracy: 0.9880 - val_loss: 1.1562 - val_decoder_dense_0_loss: 0.5628 - val_decoder_dense_1_loss: 0.0071 - val_decoder_dense_2_loss: 0.0725 - val_decoder_dense_3_loss: 0.0198 - val_decoder_dense_4_loss: 0.0202 - val_decoder_dense_5_loss: 0.0033 - val_decoder_dense_6_loss: 0.0126 - val_decoder_dense_7_loss: 0.0057 - val_decoder_dense_8_loss: 0.0139 - val_decoder_dense_9_loss: 0.0146 - val_decoder_dense_10_loss: 0.0218 - val_decoder_dense_11_loss: 0.0136 - val_decoder_dense_12_loss: 0.0038 - val_decoder_dense_13_loss: 0.0375 - val_decoder_dense_14_loss: 0.0073 - val_decoder_dense_15_loss: 0.0200 - val_decoder_dense_16_loss: 0.0083 - val_decoder_dense_17_loss: 0.0080 - val_decoder_dense_18_loss: 0.0037 - val_decoder_dense_19_loss: 0.1150 - val_decoder_dense_20_loss: 0.0570 - val_decoder_dense_0_accuracy: 0.8938 - val_decoder_dense_1_accuracy: 0.9980 - val_decoder_dense_2_accuracy: 0.9757 - val_decoder_dense_3_accuracy: 0.9913 - val_decoder_dense_4_accuracy: 0.9917 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9985 - val_decoder_dense_7_accuracy: 0.9979 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9944 - val_decoder_dense_10_accuracy: 0.9969 - val_decoder_dense_11_accuracy: 0.9941 - val_decoder_dense_12_accuracy: 0.9992 - val_decoder_dense_13_accuracy: 0.9917 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9981 - val_decoder_dense_17_accuracy: 0.9968 - val_decoder_dense_18_accuracy: 0.9991 - val_decoder_dense_19_accuracy: 0.9665 - val_decoder_dense_20_accuracy: 0.9809 - lr: 0.0049\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 1165s 29s/step - loss: 1.0444 - decoder_dense_0_loss: 0.5171 - decoder_dense_1_loss: 0.0066 - decoder_dense_2_loss: 0.0686 - decoder_dense_3_loss: 0.0166 - decoder_dense_4_loss: 0.0180 - decoder_dense_5_loss: 0.0026 - decoder_dense_6_loss: 0.0137 - decoder_dense_7_loss: 0.0051 - decoder_dense_8_loss: 0.0086 - decoder_dense_9_loss: 0.0132 - decoder_dense_10_loss: 0.0176 - decoder_dense_11_loss: 0.0099 - decoder_dense_12_loss: 0.0021 - decoder_dense_13_loss: 0.0284 - decoder_dense_14_loss: 0.0061 - decoder_dense_15_loss: 0.0135 - decoder_dense_16_loss: 0.0069 - decoder_dense_17_loss: 0.0073 - decoder_dense_18_loss: 0.0038 - decoder_dense_19_loss: 0.1095 - decoder_dense_20_loss: 0.0511 - decoder_dense_0_accuracy: 0.8985 - decoder_dense_1_accuracy: 0.9983 - decoder_dense_2_accuracy: 0.9768 - decoder_dense_3_accuracy: 0.9936 - decoder_dense_4_accuracy: 0.9932 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9972 - decoder_dense_7_accuracy: 0.9985 - decoder_dense_8_accuracy: 0.9982 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9970 - decoder_dense_11_accuracy: 0.9976 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9934 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9964 - decoder_dense_16_accuracy: 0.9982 - decoder_dense_17_accuracy: 0.9980 - decoder_dense_18_accuracy: 0.9990 - decoder_dense_19_accuracy: 0.9684 - decoder_dense_20_accuracy: 0.9886 - val_loss: 1.0426 - val_decoder_dense_0_loss: 0.5447 - val_decoder_dense_1_loss: 0.0064 - val_decoder_dense_2_loss: 0.0643 - val_decoder_dense_3_loss: 0.0159 - val_decoder_dense_4_loss: 0.0169 - val_decoder_dense_5_loss: 0.0034 - val_decoder_dense_6_loss: 0.0077 - val_decoder_dense_7_loss: 0.0057 - val_decoder_dense_8_loss: 0.0150 - val_decoder_dense_9_loss: 0.0135 - val_decoder_dense_10_loss: 0.0098 - val_decoder_dense_11_loss: 0.0107 - val_decoder_dense_12_loss: 0.0024 - val_decoder_dense_13_loss: 0.0353 - val_decoder_dense_14_loss: 0.0072 - val_decoder_dense_15_loss: 0.0183 - val_decoder_dense_16_loss: 0.0086 - val_decoder_dense_17_loss: 0.0044 - val_decoder_dense_18_loss: 0.0034 - val_decoder_dense_19_loss: 0.1071 - val_decoder_dense_20_loss: 0.0333 - val_decoder_dense_0_accuracy: 0.8993 - val_decoder_dense_1_accuracy: 0.9981 - val_decoder_dense_2_accuracy: 0.9770 - val_decoder_dense_3_accuracy: 0.9941 - val_decoder_dense_4_accuracy: 0.9936 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9991 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9970 - val_decoder_dense_9_accuracy: 0.9954 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9975 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9919 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9953 - val_decoder_dense_16_accuracy: 0.9981 - val_decoder_dense_17_accuracy: 0.9985 - val_decoder_dense_18_accuracy: 0.9991 - val_decoder_dense_19_accuracy: 0.9687 - val_decoder_dense_20_accuracy: 0.9961 - lr: 0.0049\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 1250s 31s/step - loss: 0.9523 - decoder_dense_0_loss: 0.4914 - decoder_dense_1_loss: 0.0061 - decoder_dense_2_loss: 0.0580 - decoder_dense_3_loss: 0.0138 - decoder_dense_4_loss: 0.0152 - decoder_dense_5_loss: 0.0025 - decoder_dense_6_loss: 0.0103 - decoder_dense_7_loss: 0.0049 - decoder_dense_8_loss: 0.0077 - decoder_dense_9_loss: 0.0130 - decoder_dense_10_loss: 0.0119 - decoder_dense_11_loss: 0.0088 - decoder_dense_12_loss: 0.0019 - decoder_dense_13_loss: 0.0273 - decoder_dense_14_loss: 0.0060 - decoder_dense_15_loss: 0.0136 - decoder_dense_16_loss: 0.0062 - decoder_dense_17_loss: 0.0059 - decoder_dense_18_loss: 0.0031 - decoder_dense_19_loss: 0.1062 - decoder_dense_20_loss: 0.0392 - decoder_dense_0_accuracy: 0.9019 - decoder_dense_1_accuracy: 0.9984 - decoder_dense_2_accuracy: 0.9801 - decoder_dense_3_accuracy: 0.9948 - decoder_dense_4_accuracy: 0.9942 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9985 - decoder_dense_7_accuracy: 0.9986 - decoder_dense_8_accuracy: 0.9984 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9989 - decoder_dense_11_accuracy: 0.9981 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9933 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9986 - decoder_dense_17_accuracy: 0.9985 - decoder_dense_18_accuracy: 0.9994 - decoder_dense_19_accuracy: 0.9685 - decoder_dense_20_accuracy: 0.9944 - val_loss: 0.9655 - val_decoder_dense_0_loss: 0.5171 - val_decoder_dense_1_loss: 0.0057 - val_decoder_dense_2_loss: 0.0536 - val_decoder_dense_3_loss: 0.0096 - val_decoder_dense_4_loss: 0.0157 - val_decoder_dense_5_loss: 0.0033 - val_decoder_dense_6_loss: 0.0067 - val_decoder_dense_7_loss: 0.0063 - val_decoder_dense_8_loss: 0.0152 - val_decoder_dense_9_loss: 0.0136 - val_decoder_dense_10_loss: 0.0071 - val_decoder_dense_11_loss: 0.0095 - val_decoder_dense_12_loss: 0.0037 - val_decoder_dense_13_loss: 0.0338 - val_decoder_dense_14_loss: 0.0074 - val_decoder_dense_15_loss: 0.0163 - val_decoder_dense_16_loss: 0.0075 - val_decoder_dense_17_loss: 0.0038 - val_decoder_dense_18_loss: 0.0029 - val_decoder_dense_19_loss: 0.1084 - val_decoder_dense_20_loss: 0.0282 - val_decoder_dense_0_accuracy: 0.9026 - val_decoder_dense_1_accuracy: 0.9981 - val_decoder_dense_2_accuracy: 0.9825 - val_decoder_dense_3_accuracy: 0.9962 - val_decoder_dense_4_accuracy: 0.9938 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9990 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9953 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9976 - val_decoder_dense_12_accuracy: 0.9992 - val_decoder_dense_13_accuracy: 0.9921 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9951 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9994 - val_decoder_dense_19_accuracy: 0.9683 - val_decoder_dense_20_accuracy: 0.9962 - lr: 0.0049\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 1166s 29s/step - loss: 0.8937 - decoder_dense_0_loss: 0.4687 - decoder_dense_1_loss: 0.0058 - decoder_dense_2_loss: 0.0482 - decoder_dense_3_loss: 0.0103 - decoder_dense_4_loss: 0.0139 - decoder_dense_5_loss: 0.0025 - decoder_dense_6_loss: 0.0101 - decoder_dense_7_loss: 0.0052 - decoder_dense_8_loss: 0.0074 - decoder_dense_9_loss: 0.0134 - decoder_dense_10_loss: 0.0140 - decoder_dense_11_loss: 0.0086 - decoder_dense_12_loss: 0.0020 - decoder_dense_13_loss: 0.0257 - decoder_dense_14_loss: 0.0060 - decoder_dense_15_loss: 0.0136 - decoder_dense_16_loss: 0.0059 - decoder_dense_17_loss: 0.0053 - decoder_dense_18_loss: 0.0032 - decoder_dense_19_loss: 0.1042 - decoder_dense_20_loss: 0.0359 - decoder_dense_0_accuracy: 0.9052 - decoder_dense_1_accuracy: 0.9984 - decoder_dense_2_accuracy: 0.9838 - decoder_dense_3_accuracy: 0.9963 - decoder_dense_4_accuracy: 0.9949 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9985 - decoder_dense_7_accuracy: 0.9985 - decoder_dense_8_accuracy: 0.9984 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9975 - decoder_dense_11_accuracy: 0.9980 - decoder_dense_12_accuracy: 0.9996 - decoder_dense_13_accuracy: 0.9934 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9964 - decoder_dense_16_accuracy: 0.9985 - decoder_dense_17_accuracy: 0.9988 - decoder_dense_18_accuracy: 0.9993 - decoder_dense_19_accuracy: 0.9684 - decoder_dense_20_accuracy: 0.9942 - val_loss: 4.3854 - val_decoder_dense_0_loss: 0.7246 - val_decoder_dense_1_loss: 0.0817 - val_decoder_dense_2_loss: 0.3106 - val_decoder_dense_3_loss: 0.6192 - val_decoder_dense_4_loss: 0.0644 - val_decoder_dense_5_loss: 0.0905 - val_decoder_dense_6_loss: 0.2519 - val_decoder_dense_7_loss: 0.1022 - val_decoder_dense_8_loss: 0.1493 - val_decoder_dense_9_loss: 0.0961 - val_decoder_dense_10_loss: 0.3191 - val_decoder_dense_11_loss: 0.0880 - val_decoder_dense_12_loss: 0.0925 - val_decoder_dense_13_loss: 0.1575 - val_decoder_dense_14_loss: 0.0645 - val_decoder_dense_15_loss: 0.0845 - val_decoder_dense_16_loss: 0.1421 - val_decoder_dense_17_loss: 0.0846 - val_decoder_dense_18_loss: 0.0539 - val_decoder_dense_19_loss: 0.5551 - val_decoder_dense_20_loss: 0.1747 - val_decoder_dense_0_accuracy: 0.8758 - val_decoder_dense_1_accuracy: 0.9773 - val_decoder_dense_2_accuracy: 0.9375 - val_decoder_dense_3_accuracy: 0.9320 - val_decoder_dense_4_accuracy: 0.9854 - val_decoder_dense_5_accuracy: 0.9791 - val_decoder_dense_6_accuracy: 0.9466 - val_decoder_dense_7_accuracy: 0.9852 - val_decoder_dense_8_accuracy: 0.9658 - val_decoder_dense_9_accuracy: 0.9919 - val_decoder_dense_10_accuracy: 0.9442 - val_decoder_dense_11_accuracy: 0.9777 - val_decoder_dense_12_accuracy: 0.9661 - val_decoder_dense_13_accuracy: 0.9711 - val_decoder_dense_14_accuracy: 0.9916 - val_decoder_dense_15_accuracy: 0.9898 - val_decoder_dense_16_accuracy: 0.9720 - val_decoder_dense_17_accuracy: 0.9737 - val_decoder_dense_18_accuracy: 0.9808 - val_decoder_dense_19_accuracy: 0.9184 - val_decoder_dense_20_accuracy: 0.9563 - lr: 0.0049\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 1165s 29s/step - loss: 1.1480 - decoder_dense_0_loss: 0.4799 - decoder_dense_1_loss: 0.0076 - decoder_dense_2_loss: 0.0545 - decoder_dense_3_loss: 0.0128 - decoder_dense_4_loss: 0.0157 - decoder_dense_5_loss: 0.0039 - decoder_dense_6_loss: 0.0265 - decoder_dense_7_loss: 0.0065 - decoder_dense_8_loss: 0.0140 - decoder_dense_9_loss: 0.0158 - decoder_dense_10_loss: 0.0335 - decoder_dense_11_loss: 0.0144 - decoder_dense_12_loss: 0.0132 - decoder_dense_13_loss: 0.0388 - decoder_dense_14_loss: 0.0066 - decoder_dense_15_loss: 0.0143 - decoder_dense_16_loss: 0.0148 - decoder_dense_17_loss: 0.0135 - decoder_dense_18_loss: 0.0090 - decoder_dense_19_loss: 0.1311 - decoder_dense_20_loss: 0.0917 - decoder_dense_0_accuracy: 0.9031 - decoder_dense_1_accuracy: 0.9978 - decoder_dense_2_accuracy: 0.9815 - decoder_dense_3_accuracy: 0.9955 - decoder_dense_4_accuracy: 0.9943 - decoder_dense_5_accuracy: 0.9992 - decoder_dense_6_accuracy: 0.9924 - decoder_dense_7_accuracy: 0.9983 - decoder_dense_8_accuracy: 0.9962 - decoder_dense_9_accuracy: 0.9949 - decoder_dense_10_accuracy: 0.9900 - decoder_dense_11_accuracy: 0.9962 - decoder_dense_12_accuracy: 0.9964 - decoder_dense_13_accuracy: 0.9904 - decoder_dense_14_accuracy: 0.9984 - decoder_dense_15_accuracy: 0.9963 - decoder_dense_16_accuracy: 0.9959 - decoder_dense_17_accuracy: 0.9961 - decoder_dense_18_accuracy: 0.9973 - decoder_dense_19_accuracy: 0.9635 - decoder_dense_20_accuracy: 0.9705 - val_loss: 1.0657 - val_decoder_dense_0_loss: 0.5051 - val_decoder_dense_1_loss: 0.0057 - val_decoder_dense_2_loss: 0.0418 - val_decoder_dense_3_loss: 0.0070 - val_decoder_dense_4_loss: 0.0159 - val_decoder_dense_5_loss: 0.0034 - val_decoder_dense_6_loss: 0.0091 - val_decoder_dense_7_loss: 0.0061 - val_decoder_dense_8_loss: 0.0161 - val_decoder_dense_9_loss: 0.0155 - val_decoder_dense_10_loss: 0.0149 - val_decoder_dense_11_loss: 0.0097 - val_decoder_dense_12_loss: 0.0039 - val_decoder_dense_13_loss: 0.0376 - val_decoder_dense_14_loss: 0.0073 - val_decoder_dense_15_loss: 0.0193 - val_decoder_dense_16_loss: 0.0100 - val_decoder_dense_17_loss: 0.0072 - val_decoder_dense_18_loss: 0.0045 - val_decoder_dense_19_loss: 0.1124 - val_decoder_dense_20_loss: 0.0647 - val_decoder_dense_0_accuracy: 0.9055 - val_decoder_dense_1_accuracy: 0.9983 - val_decoder_dense_2_accuracy: 0.9868 - val_decoder_dense_3_accuracy: 0.9971 - val_decoder_dense_4_accuracy: 0.9937 - val_decoder_dense_5_accuracy: 0.9994 - val_decoder_dense_6_accuracy: 0.9992 - val_decoder_dense_7_accuracy: 0.9985 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9987 - val_decoder_dense_11_accuracy: 0.9983 - val_decoder_dense_12_accuracy: 0.9994 - val_decoder_dense_13_accuracy: 0.9916 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9975 - val_decoder_dense_17_accuracy: 0.9985 - val_decoder_dense_18_accuracy: 0.9991 - val_decoder_dense_19_accuracy: 0.9676 - val_decoder_dense_20_accuracy: 0.9789 - lr: 0.0049\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 1148s 29s/step - loss: 0.9387 - decoder_dense_0_loss: 0.4440 - decoder_dense_1_loss: 0.0058 - decoder_dense_2_loss: 0.0404 - decoder_dense_3_loss: 0.0073 - decoder_dense_4_loss: 0.0135 - decoder_dense_5_loss: 0.0023 - decoder_dense_6_loss: 0.0112 - decoder_dense_7_loss: 0.0051 - decoder_dense_8_loss: 0.0073 - decoder_dense_9_loss: 0.0136 - decoder_dense_10_loss: 0.0157 - decoder_dense_11_loss: 0.0083 - decoder_dense_12_loss: 0.0029 - decoder_dense_13_loss: 0.0274 - decoder_dense_14_loss: 0.0057 - decoder_dense_15_loss: 0.0132 - decoder_dense_16_loss: 0.0080 - decoder_dense_17_loss: 0.0079 - decoder_dense_18_loss: 0.0053 - decoder_dense_19_loss: 0.1055 - decoder_dense_20_loss: 0.0568 - decoder_dense_0_accuracy: 0.9089 - decoder_dense_1_accuracy: 0.9984 - decoder_dense_2_accuracy: 0.9864 - decoder_dense_3_accuracy: 0.9972 - decoder_dense_4_accuracy: 0.9951 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9982 - decoder_dense_7_accuracy: 0.9985 - decoder_dense_8_accuracy: 0.9985 - decoder_dense_9_accuracy: 0.9955 - decoder_dense_10_accuracy: 0.9968 - decoder_dense_11_accuracy: 0.9984 - decoder_dense_12_accuracy: 0.9995 - decoder_dense_13_accuracy: 0.9931 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9977 - decoder_dense_17_accuracy: 0.9978 - decoder_dense_18_accuracy: 0.9986 - decoder_dense_19_accuracy: 0.9683 - decoder_dense_20_accuracy: 0.9847 - val_loss: 0.9313 - val_decoder_dense_0_loss: 0.4757 - val_decoder_dense_1_loss: 0.0049 - val_decoder_dense_2_loss: 0.0371 - val_decoder_dense_3_loss: 0.0060 - val_decoder_dense_4_loss: 0.0156 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0051 - val_decoder_dense_7_loss: 0.0056 - val_decoder_dense_8_loss: 0.0154 - val_decoder_dense_9_loss: 0.0148 - val_decoder_dense_10_loss: 0.0099 - val_decoder_dense_11_loss: 0.0071 - val_decoder_dense_12_loss: 0.0021 - val_decoder_dense_13_loss: 0.0337 - val_decoder_dense_14_loss: 0.0070 - val_decoder_dense_15_loss: 0.0170 - val_decoder_dense_16_loss: 0.0082 - val_decoder_dense_17_loss: 0.0052 - val_decoder_dense_18_loss: 0.0032 - val_decoder_dense_19_loss: 0.1023 - val_decoder_dense_20_loss: 0.0363 - val_decoder_dense_0_accuracy: 0.9096 - val_decoder_dense_1_accuracy: 0.9985 - val_decoder_dense_2_accuracy: 0.9881 - val_decoder_dense_3_accuracy: 0.9971 - val_decoder_dense_4_accuracy: 0.9937 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9995 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9994 - val_decoder_dense_11_accuracy: 0.9988 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9921 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9952 - val_decoder_dense_16_accuracy: 0.9981 - val_decoder_dense_17_accuracy: 0.9987 - val_decoder_dense_18_accuracy: 0.9992 - val_decoder_dense_19_accuracy: 0.9684 - val_decoder_dense_20_accuracy: 0.9947 - lr: 0.0049\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 1178s 29s/step - loss: 0.8485 - decoder_dense_0_loss: 0.4270 - decoder_dense_1_loss: 0.0052 - decoder_dense_2_loss: 0.0341 - decoder_dense_3_loss: 0.0061 - decoder_dense_4_loss: 0.0123 - decoder_dense_5_loss: 0.0022 - decoder_dense_6_loss: 0.0092 - decoder_dense_7_loss: 0.0046 - decoder_dense_8_loss: 0.0068 - decoder_dense_9_loss: 0.0127 - decoder_dense_10_loss: 0.0109 - decoder_dense_11_loss: 0.0068 - decoder_dense_12_loss: 0.0019 - decoder_dense_13_loss: 0.0248 - decoder_dense_14_loss: 0.0054 - decoder_dense_15_loss: 0.0129 - decoder_dense_16_loss: 0.0061 - decoder_dense_17_loss: 0.0061 - decoder_dense_18_loss: 0.0039 - decoder_dense_19_loss: 0.1002 - decoder_dense_20_loss: 0.0429 - decoder_dense_0_accuracy: 0.9115 - decoder_dense_1_accuracy: 0.9985 - decoder_dense_2_accuracy: 0.9886 - decoder_dense_3_accuracy: 0.9977 - decoder_dense_4_accuracy: 0.9956 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9986 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9986 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9979 - decoder_dense_11_accuracy: 0.9985 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9935 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9965 - decoder_dense_16_accuracy: 0.9983 - decoder_dense_17_accuracy: 0.9983 - decoder_dense_18_accuracy: 0.9990 - decoder_dense_19_accuracy: 0.9697 - decoder_dense_20_accuracy: 0.9911 - val_loss: 0.8992 - val_decoder_dense_0_loss: 0.4691 - val_decoder_dense_1_loss: 0.0045 - val_decoder_dense_2_loss: 0.0341 - val_decoder_dense_3_loss: 0.0061 - val_decoder_dense_4_loss: 0.0146 - val_decoder_dense_5_loss: 0.0032 - val_decoder_dense_6_loss: 0.0074 - val_decoder_dense_7_loss: 0.0055 - val_decoder_dense_8_loss: 0.0143 - val_decoder_dense_9_loss: 0.0141 - val_decoder_dense_10_loss: 0.0098 - val_decoder_dense_11_loss: 0.0080 - val_decoder_dense_12_loss: 0.0025 - val_decoder_dense_13_loss: 0.0328 - val_decoder_dense_14_loss: 0.0068 - val_decoder_dense_15_loss: 0.0161 - val_decoder_dense_16_loss: 0.0069 - val_decoder_dense_17_loss: 0.0051 - val_decoder_dense_18_loss: 0.0044 - val_decoder_dense_19_loss: 0.1037 - val_decoder_dense_20_loss: 0.0313 - val_decoder_dense_0_accuracy: 0.9119 - val_decoder_dense_1_accuracy: 0.9985 - val_decoder_dense_2_accuracy: 0.9888 - val_decoder_dense_3_accuracy: 0.9973 - val_decoder_dense_4_accuracy: 0.9950 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9992 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9987 - val_decoder_dense_11_accuracy: 0.9987 - val_decoder_dense_12_accuracy: 0.9994 - val_decoder_dense_13_accuracy: 0.9924 - val_decoder_dense_14_accuracy: 0.9981 - val_decoder_dense_15_accuracy: 0.9953 - val_decoder_dense_16_accuracy: 0.9982 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9698 - val_decoder_dense_20_accuracy: 0.9954 - lr: 0.0049\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 1165s 29s/step - loss: 0.8098 - decoder_dense_0_loss: 0.4156 - decoder_dense_1_loss: 0.0050 - decoder_dense_2_loss: 0.0323 - decoder_dense_3_loss: 0.0059 - decoder_dense_4_loss: 0.0118 - decoder_dense_5_loss: 0.0022 - decoder_dense_6_loss: 0.0101 - decoder_dense_7_loss: 0.0046 - decoder_dense_8_loss: 0.0074 - decoder_dense_9_loss: 0.0126 - decoder_dense_10_loss: 0.0080 - decoder_dense_11_loss: 0.0067 - decoder_dense_12_loss: 0.0015 - decoder_dense_13_loss: 0.0234 - decoder_dense_14_loss: 0.0055 - decoder_dense_15_loss: 0.0124 - decoder_dense_16_loss: 0.0054 - decoder_dense_17_loss: 0.0060 - decoder_dense_18_loss: 0.0036 - decoder_dense_19_loss: 0.0986 - decoder_dense_20_loss: 0.0367 - decoder_dense_0_accuracy: 0.9123 - decoder_dense_1_accuracy: 0.9985 - decoder_dense_2_accuracy: 0.9886 - decoder_dense_3_accuracy: 0.9978 - decoder_dense_4_accuracy: 0.9959 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9982 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9984 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9988 - decoder_dense_11_accuracy: 0.9985 - decoder_dense_12_accuracy: 0.9998 - decoder_dense_13_accuracy: 0.9937 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9966 - decoder_dense_16_accuracy: 0.9985 - decoder_dense_17_accuracy: 0.9984 - decoder_dense_18_accuracy: 0.9992 - decoder_dense_19_accuracy: 0.9699 - decoder_dense_20_accuracy: 0.9931 - val_loss: 0.8646 - val_decoder_dense_0_loss: 0.4683 - val_decoder_dense_1_loss: 0.0045 - val_decoder_dense_2_loss: 0.0318 - val_decoder_dense_3_loss: 0.0057 - val_decoder_dense_4_loss: 0.0139 - val_decoder_dense_5_loss: 0.0032 - val_decoder_dense_6_loss: 0.0048 - val_decoder_dense_7_loss: 0.0063 - val_decoder_dense_8_loss: 0.0158 - val_decoder_dense_9_loss: 0.0153 - val_decoder_dense_10_loss: 0.0048 - val_decoder_dense_11_loss: 0.0076 - val_decoder_dense_12_loss: 0.0024 - val_decoder_dense_13_loss: 0.0307 - val_decoder_dense_14_loss: 0.0079 - val_decoder_dense_15_loss: 0.0168 - val_decoder_dense_16_loss: 0.0073 - val_decoder_dense_17_loss: 0.0037 - val_decoder_dense_18_loss: 0.0027 - val_decoder_dense_19_loss: 0.0976 - val_decoder_dense_20_loss: 0.0246 - val_decoder_dense_0_accuracy: 0.9113 - val_decoder_dense_1_accuracy: 0.9989 - val_decoder_dense_2_accuracy: 0.9892 - val_decoder_dense_3_accuracy: 0.9979 - val_decoder_dense_4_accuracy: 0.9950 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9995 - val_decoder_dense_7_accuracy: 0.9981 - val_decoder_dense_8_accuracy: 0.9966 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9980 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9927 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9954 - val_decoder_dense_16_accuracy: 0.9981 - val_decoder_dense_17_accuracy: 0.9990 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9705 - val_decoder_dense_20_accuracy: 0.9965 - lr: 0.0049\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 1161s 29s/step - loss: 0.7627 - decoder_dense_0_loss: 0.4025 - decoder_dense_1_loss: 0.0043 - decoder_dense_2_loss: 0.0290 - decoder_dense_3_loss: 0.0052 - decoder_dense_4_loss: 0.0113 - decoder_dense_5_loss: 0.0022 - decoder_dense_6_loss: 0.0076 - decoder_dense_7_loss: 0.0044 - decoder_dense_8_loss: 0.0059 - decoder_dense_9_loss: 0.0125 - decoder_dense_10_loss: 0.0054 - decoder_dense_11_loss: 0.0063 - decoder_dense_12_loss: 0.0015 - decoder_dense_13_loss: 0.0226 - decoder_dense_14_loss: 0.0055 - decoder_dense_15_loss: 0.0124 - decoder_dense_16_loss: 0.0050 - decoder_dense_17_loss: 0.0049 - decoder_dense_18_loss: 0.0030 - decoder_dense_19_loss: 0.0971 - decoder_dense_20_loss: 0.0306 - decoder_dense_0_accuracy: 0.9141 - decoder_dense_1_accuracy: 0.9988 - decoder_dense_2_accuracy: 0.9899 - decoder_dense_3_accuracy: 0.9981 - decoder_dense_4_accuracy: 0.9960 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9988 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9995 - decoder_dense_11_accuracy: 0.9986 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9938 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9966 - decoder_dense_16_accuracy: 0.9986 - decoder_dense_17_accuracy: 0.9987 - decoder_dense_18_accuracy: 0.9992 - decoder_dense_19_accuracy: 0.9706 - decoder_dense_20_accuracy: 0.9948 - val_loss: 0.8254 - val_decoder_dense_0_loss: 0.4521 - val_decoder_dense_1_loss: 0.0041 - val_decoder_dense_2_loss: 0.0264 - val_decoder_dense_3_loss: 0.0048 - val_decoder_dense_4_loss: 0.0151 - val_decoder_dense_5_loss: 0.0031 - val_decoder_dense_6_loss: 0.0037 - val_decoder_dense_7_loss: 0.0053 - val_decoder_dense_8_loss: 0.0147 - val_decoder_dense_9_loss: 0.0146 - val_decoder_dense_10_loss: 0.0040 - val_decoder_dense_11_loss: 0.0060 - val_decoder_dense_12_loss: 0.0020 - val_decoder_dense_13_loss: 0.0330 - val_decoder_dense_14_loss: 0.0064 - val_decoder_dense_15_loss: 0.0166 - val_decoder_dense_16_loss: 0.0082 - val_decoder_dense_17_loss: 0.0036 - val_decoder_dense_18_loss: 0.0028 - val_decoder_dense_19_loss: 0.0999 - val_decoder_dense_20_loss: 0.0205 - val_decoder_dense_0_accuracy: 0.9144 - val_decoder_dense_1_accuracy: 0.9985 - val_decoder_dense_2_accuracy: 0.9916 - val_decoder_dense_3_accuracy: 0.9981 - val_decoder_dense_4_accuracy: 0.9950 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9995 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9989 - val_decoder_dense_12_accuracy: 0.9997 - val_decoder_dense_13_accuracy: 0.9911 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9953 - val_decoder_dense_16_accuracy: 0.9980 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9702 - val_decoder_dense_20_accuracy: 0.9975 - lr: 0.0049\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 1156s 29s/step - loss: 0.7289 - decoder_dense_0_loss: 0.3893 - decoder_dense_1_loss: 0.0039 - decoder_dense_2_loss: 0.0269 - decoder_dense_3_loss: 0.0053 - decoder_dense_4_loss: 0.0117 - decoder_dense_5_loss: 0.0021 - decoder_dense_6_loss: 0.0072 - decoder_dense_7_loss: 0.0044 - decoder_dense_8_loss: 0.0059 - decoder_dense_9_loss: 0.0121 - decoder_dense_10_loss: 0.0050 - decoder_dense_11_loss: 0.0055 - decoder_dense_12_loss: 0.0015 - decoder_dense_13_loss: 0.0224 - decoder_dense_14_loss: 0.0049 - decoder_dense_15_loss: 0.0116 - decoder_dense_16_loss: 0.0048 - decoder_dense_17_loss: 0.0050 - decoder_dense_18_loss: 0.0032 - decoder_dense_19_loss: 0.0949 - decoder_dense_20_loss: 0.0256 - decoder_dense_0_accuracy: 0.9157 - decoder_dense_1_accuracy: 0.9989 - decoder_dense_2_accuracy: 0.9906 - decoder_dense_3_accuracy: 0.9982 - decoder_dense_4_accuracy: 0.9958 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9989 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9995 - decoder_dense_11_accuracy: 0.9987 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9940 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9967 - decoder_dense_16_accuracy: 0.9989 - decoder_dense_17_accuracy: 0.9988 - decoder_dense_18_accuracy: 0.9992 - decoder_dense_19_accuracy: 0.9709 - decoder_dense_20_accuracy: 0.9963 - val_loss: 0.8081 - val_decoder_dense_0_loss: 0.4499 - val_decoder_dense_1_loss: 0.0038 - val_decoder_dense_2_loss: 0.0257 - val_decoder_dense_3_loss: 0.0053 - val_decoder_dense_4_loss: 0.0142 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0039 - val_decoder_dense_7_loss: 0.0049 - val_decoder_dense_8_loss: 0.0131 - val_decoder_dense_9_loss: 0.0141 - val_decoder_dense_10_loss: 0.0043 - val_decoder_dense_11_loss: 0.0064 - val_decoder_dense_12_loss: 0.0023 - val_decoder_dense_13_loss: 0.0295 - val_decoder_dense_14_loss: 0.0053 - val_decoder_dense_15_loss: 0.0162 - val_decoder_dense_16_loss: 0.0073 - val_decoder_dense_17_loss: 0.0025 - val_decoder_dense_18_loss: 0.0028 - val_decoder_dense_19_loss: 0.1041 - val_decoder_dense_20_loss: 0.0167 - val_decoder_dense_0_accuracy: 0.9142 - val_decoder_dense_1_accuracy: 0.9988 - val_decoder_dense_2_accuracy: 0.9913 - val_decoder_dense_3_accuracy: 0.9981 - val_decoder_dense_4_accuracy: 0.9953 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9993 - val_decoder_dense_7_accuracy: 0.9984 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9996 - val_decoder_dense_11_accuracy: 0.9987 - val_decoder_dense_12_accuracy: 0.9995 - val_decoder_dense_13_accuracy: 0.9927 - val_decoder_dense_14_accuracy: 0.9983 - val_decoder_dense_15_accuracy: 0.9953 - val_decoder_dense_16_accuracy: 0.9982 - val_decoder_dense_17_accuracy: 0.9993 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9690 - val_decoder_dense_20_accuracy: 0.9982 - lr: 0.0049\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 1153s 29s/step - loss: 0.7132 - decoder_dense_0_loss: 0.3814 - decoder_dense_1_loss: 0.0040 - decoder_dense_2_loss: 0.0240 - decoder_dense_3_loss: 0.0057 - decoder_dense_4_loss: 0.0112 - decoder_dense_5_loss: 0.0021 - decoder_dense_6_loss: 0.0074 - decoder_dense_7_loss: 0.0043 - decoder_dense_8_loss: 0.0055 - decoder_dense_9_loss: 0.0129 - decoder_dense_10_loss: 0.0048 - decoder_dense_11_loss: 0.0058 - decoder_dense_12_loss: 0.0017 - decoder_dense_13_loss: 0.0215 - decoder_dense_14_loss: 0.0047 - decoder_dense_15_loss: 0.0110 - decoder_dense_16_loss: 0.0057 - decoder_dense_17_loss: 0.0049 - decoder_dense_18_loss: 0.0033 - decoder_dense_19_loss: 0.0964 - decoder_dense_20_loss: 0.0240 - decoder_dense_0_accuracy: 0.9167 - decoder_dense_1_accuracy: 0.9987 - decoder_dense_2_accuracy: 0.9919 - decoder_dense_3_accuracy: 0.9980 - decoder_dense_4_accuracy: 0.9962 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9987 - decoder_dense_7_accuracy: 0.9988 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9955 - decoder_dense_10_accuracy: 0.9994 - decoder_dense_11_accuracy: 0.9987 - decoder_dense_12_accuracy: 0.9996 - decoder_dense_13_accuracy: 0.9940 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9968 - decoder_dense_16_accuracy: 0.9984 - decoder_dense_17_accuracy: 0.9986 - decoder_dense_18_accuracy: 0.9993 - decoder_dense_19_accuracy: 0.9704 - decoder_dense_20_accuracy: 0.9969 - val_loss: 0.7948 - val_decoder_dense_0_loss: 0.4418 - val_decoder_dense_1_loss: 0.0037 - val_decoder_dense_2_loss: 0.0254 - val_decoder_dense_3_loss: 0.0070 - val_decoder_dense_4_loss: 0.0151 - val_decoder_dense_5_loss: 0.0031 - val_decoder_dense_6_loss: 0.0031 - val_decoder_dense_7_loss: 0.0048 - val_decoder_dense_8_loss: 0.0138 - val_decoder_dense_9_loss: 0.0139 - val_decoder_dense_10_loss: 0.0035 - val_decoder_dense_11_loss: 0.0062 - val_decoder_dense_12_loss: 0.0024 - val_decoder_dense_13_loss: 0.0295 - val_decoder_dense_14_loss: 0.0054 - val_decoder_dense_15_loss: 0.0146 - val_decoder_dense_16_loss: 0.0080 - val_decoder_dense_17_loss: 0.0047 - val_decoder_dense_18_loss: 0.0036 - val_decoder_dense_19_loss: 0.0979 - val_decoder_dense_20_loss: 0.0171 - val_decoder_dense_0_accuracy: 0.9159 - val_decoder_dense_1_accuracy: 0.9987 - val_decoder_dense_2_accuracy: 0.9909 - val_decoder_dense_3_accuracy: 0.9971 - val_decoder_dense_4_accuracy: 0.9953 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9995 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9969 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9986 - val_decoder_dense_12_accuracy: 0.9995 - val_decoder_dense_13_accuracy: 0.9927 - val_decoder_dense_14_accuracy: 0.9983 - val_decoder_dense_15_accuracy: 0.9960 - val_decoder_dense_16_accuracy: 0.9982 - val_decoder_dense_17_accuracy: 0.9987 - val_decoder_dense_18_accuracy: 0.9992 - val_decoder_dense_19_accuracy: 0.9705 - val_decoder_dense_20_accuracy: 0.9976 - lr: 0.0049\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 1141s 29s/step - loss: 0.6915 - decoder_dense_0_loss: 0.3715 - decoder_dense_1_loss: 0.0034 - decoder_dense_2_loss: 0.0230 - decoder_dense_3_loss: 0.0049 - decoder_dense_4_loss: 0.0107 - decoder_dense_5_loss: 0.0021 - decoder_dense_6_loss: 0.0066 - decoder_dense_7_loss: 0.0047 - decoder_dense_8_loss: 0.0058 - decoder_dense_9_loss: 0.0128 - decoder_dense_10_loss: 0.0042 - decoder_dense_11_loss: 0.0056 - decoder_dense_12_loss: 0.0016 - decoder_dense_13_loss: 0.0206 - decoder_dense_14_loss: 0.0043 - decoder_dense_15_loss: 0.0102 - decoder_dense_16_loss: 0.0051 - decoder_dense_17_loss: 0.0051 - decoder_dense_18_loss: 0.0033 - decoder_dense_19_loss: 0.0943 - decoder_dense_20_loss: 0.0226 - decoder_dense_0_accuracy: 0.9189 - decoder_dense_1_accuracy: 0.9991 - decoder_dense_2_accuracy: 0.9923 - decoder_dense_3_accuracy: 0.9984 - decoder_dense_4_accuracy: 0.9962 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9989 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9995 - decoder_dense_11_accuracy: 0.9985 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9943 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9970 - decoder_dense_16_accuracy: 0.9987 - decoder_dense_17_accuracy: 0.9987 - decoder_dense_18_accuracy: 0.9993 - decoder_dense_19_accuracy: 0.9710 - decoder_dense_20_accuracy: 0.9971 - val_loss: 0.7701 - val_decoder_dense_0_loss: 0.4325 - val_decoder_dense_1_loss: 0.0036 - val_decoder_dense_2_loss: 0.0201 - val_decoder_dense_3_loss: 0.0044 - val_decoder_dense_4_loss: 0.0126 - val_decoder_dense_5_loss: 0.0031 - val_decoder_dense_6_loss: 0.0053 - val_decoder_dense_7_loss: 0.0051 - val_decoder_dense_8_loss: 0.0158 - val_decoder_dense_9_loss: 0.0143 - val_decoder_dense_10_loss: 0.0033 - val_decoder_dense_11_loss: 0.0063 - val_decoder_dense_12_loss: 0.0025 - val_decoder_dense_13_loss: 0.0287 - val_decoder_dense_14_loss: 0.0046 - val_decoder_dense_15_loss: 0.0146 - val_decoder_dense_16_loss: 0.0075 - val_decoder_dense_17_loss: 0.0035 - val_decoder_dense_18_loss: 0.0028 - val_decoder_dense_19_loss: 0.0989 - val_decoder_dense_20_loss: 0.0135 - val_decoder_dense_0_accuracy: 0.9189 - val_decoder_dense_1_accuracy: 0.9988 - val_decoder_dense_2_accuracy: 0.9932 - val_decoder_dense_3_accuracy: 0.9987 - val_decoder_dense_4_accuracy: 0.9959 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9987 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9952 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9990 - val_decoder_dense_12_accuracy: 0.9995 - val_decoder_dense_13_accuracy: 0.9933 - val_decoder_dense_14_accuracy: 0.9986 - val_decoder_dense_15_accuracy: 0.9961 - val_decoder_dense_16_accuracy: 0.9985 - val_decoder_dense_17_accuracy: 0.9992 - val_decoder_dense_18_accuracy: 0.9996 - val_decoder_dense_19_accuracy: 0.9703 - val_decoder_dense_20_accuracy: 0.9987 - lr: 0.0049\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 1157s 29s/step - loss: 0.6772 - decoder_dense_0_loss: 0.3637 - decoder_dense_1_loss: 0.0036 - decoder_dense_2_loss: 0.0225 - decoder_dense_3_loss: 0.0048 - decoder_dense_4_loss: 0.0104 - decoder_dense_5_loss: 0.0020 - decoder_dense_6_loss: 0.0069 - decoder_dense_7_loss: 0.0043 - decoder_dense_8_loss: 0.0064 - decoder_dense_9_loss: 0.0123 - decoder_dense_10_loss: 0.0069 - decoder_dense_11_loss: 0.0058 - decoder_dense_12_loss: 0.0013 - decoder_dense_13_loss: 0.0203 - decoder_dense_14_loss: 0.0041 - decoder_dense_15_loss: 0.0095 - decoder_dense_16_loss: 0.0043 - decoder_dense_17_loss: 0.0048 - decoder_dense_18_loss: 0.0028 - decoder_dense_19_loss: 0.0933 - decoder_dense_20_loss: 0.0211 - decoder_dense_0_accuracy: 0.9198 - decoder_dense_1_accuracy: 0.9989 - decoder_dense_2_accuracy: 0.9919 - decoder_dense_3_accuracy: 0.9983 - decoder_dense_4_accuracy: 0.9965 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9987 - decoder_dense_7_accuracy: 0.9988 - decoder_dense_8_accuracy: 0.9985 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9985 - decoder_dense_11_accuracy: 0.9987 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9944 - decoder_dense_14_accuracy: 0.9985 - decoder_dense_15_accuracy: 0.9974 - decoder_dense_16_accuracy: 0.9989 - decoder_dense_17_accuracy: 0.9990 - decoder_dense_18_accuracy: 0.9994 - decoder_dense_19_accuracy: 0.9714 - decoder_dense_20_accuracy: 0.9972 - val_loss: 0.7637 - val_decoder_dense_0_loss: 0.4291 - val_decoder_dense_1_loss: 0.0049 - val_decoder_dense_2_loss: 0.0221 - val_decoder_dense_3_loss: 0.0045 - val_decoder_dense_4_loss: 0.0133 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0030 - val_decoder_dense_7_loss: 0.0049 - val_decoder_dense_8_loss: 0.0145 - val_decoder_dense_9_loss: 0.0132 - val_decoder_dense_10_loss: 0.0043 - val_decoder_dense_11_loss: 0.0049 - val_decoder_dense_12_loss: 0.0023 - val_decoder_dense_13_loss: 0.0281 - val_decoder_dense_14_loss: 0.0049 - val_decoder_dense_15_loss: 0.0125 - val_decoder_dense_16_loss: 0.0077 - val_decoder_dense_17_loss: 0.0037 - val_decoder_dense_18_loss: 0.0026 - val_decoder_dense_19_loss: 0.0986 - val_decoder_dense_20_loss: 0.0144 - val_decoder_dense_0_accuracy: 0.9181 - val_decoder_dense_1_accuracy: 0.9982 - val_decoder_dense_2_accuracy: 0.9928 - val_decoder_dense_3_accuracy: 0.9983 - val_decoder_dense_4_accuracy: 0.9955 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9992 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9954 - val_decoder_dense_10_accuracy: 0.9996 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9995 - val_decoder_dense_13_accuracy: 0.9931 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9965 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9994 - val_decoder_dense_19_accuracy: 0.9708 - val_decoder_dense_20_accuracy: 0.9971 - lr: 0.0049\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 1155s 29s/step - loss: 0.6533 - decoder_dense_0_loss: 0.3530 - decoder_dense_1_loss: 0.0033 - decoder_dense_2_loss: 0.0198 - decoder_dense_3_loss: 0.0036 - decoder_dense_4_loss: 0.0095 - decoder_dense_5_loss: 0.0019 - decoder_dense_6_loss: 0.0067 - decoder_dense_7_loss: 0.0042 - decoder_dense_8_loss: 0.0056 - decoder_dense_9_loss: 0.0120 - decoder_dense_10_loss: 0.0041 - decoder_dense_11_loss: 0.0044 - decoder_dense_12_loss: 0.0011 - decoder_dense_13_loss: 0.0202 - decoder_dense_14_loss: 0.0038 - decoder_dense_15_loss: 0.0088 - decoder_dense_16_loss: 0.0043 - decoder_dense_17_loss: 0.0050 - decoder_dense_18_loss: 0.0028 - decoder_dense_19_loss: 0.0936 - decoder_dense_20_loss: 0.0195 - decoder_dense_0_accuracy: 0.9210 - decoder_dense_1_accuracy: 0.9990 - decoder_dense_2_accuracy: 0.9931 - decoder_dense_3_accuracy: 0.9989 - decoder_dense_4_accuracy: 0.9966 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9989 - decoder_dense_7_accuracy: 0.9988 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9995 - decoder_dense_11_accuracy: 0.9990 - decoder_dense_12_accuracy: 0.9998 - decoder_dense_13_accuracy: 0.9945 - decoder_dense_14_accuracy: 0.9987 - decoder_dense_15_accuracy: 0.9974 - decoder_dense_16_accuracy: 0.9988 - decoder_dense_17_accuracy: 0.9987 - decoder_dense_18_accuracy: 0.9993 - decoder_dense_19_accuracy: 0.9713 - decoder_dense_20_accuracy: 0.9976 - val_loss: 0.7391 - val_decoder_dense_0_loss: 0.4223 - val_decoder_dense_1_loss: 0.0031 - val_decoder_dense_2_loss: 0.0174 - val_decoder_dense_3_loss: 0.0029 - val_decoder_dense_4_loss: 0.0125 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0021 - val_decoder_dense_7_loss: 0.0058 - val_decoder_dense_8_loss: 0.0162 - val_decoder_dense_9_loss: 0.0158 - val_decoder_dense_10_loss: 0.0031 - val_decoder_dense_11_loss: 0.0043 - val_decoder_dense_12_loss: 0.0018 - val_decoder_dense_13_loss: 0.0275 - val_decoder_dense_14_loss: 0.0062 - val_decoder_dense_15_loss: 0.0108 - val_decoder_dense_16_loss: 0.0063 - val_decoder_dense_17_loss: 0.0031 - val_decoder_dense_18_loss: 0.0026 - val_decoder_dense_19_loss: 0.0975 - val_decoder_dense_20_loss: 0.0109 - val_decoder_dense_0_accuracy: 0.9197 - val_decoder_dense_1_accuracy: 0.9989 - val_decoder_dense_2_accuracy: 0.9937 - val_decoder_dense_3_accuracy: 0.9995 - val_decoder_dense_4_accuracy: 0.9962 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9997 - val_decoder_dense_7_accuracy: 0.9984 - val_decoder_dense_8_accuracy: 0.9967 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9933 - val_decoder_dense_14_accuracy: 0.9978 - val_decoder_dense_15_accuracy: 0.9975 - val_decoder_dense_16_accuracy: 0.9981 - val_decoder_dense_17_accuracy: 0.9992 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9701 - val_decoder_dense_20_accuracy: 0.9988 - lr: 0.0049\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7963ce7b23b0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit([X_train, y_train], [y_train[:, i, :] for i in range(y_train.shape[1])], epochs=20, batch_size= 32,\n",
        "             validation_split=0.2, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FnvfaoULFD0J"
      },
      "outputs": [],
      "source": [
        "# model.save('model.h5')\n",
        "\n",
        "model.save('my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1vAjivipf1d"
      },
      "outputs": [],
      "source": [
        "input_sentence = \"Ho flatulence since one year . Abdomen problem . Complains from more than 1 year ago . Gradually it happen . No attack . Patient does not have smoking habit . Does not drink alcohol . \"\n",
        "input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "decoder_input_sequence = np.zeros((1, 21, y_max_sequence_length))\n",
        "\n",
        "predictions = model.predict([input_sequence, decoder_input_sequence])\n",
        "\n",
        "predicted_sentences = []\n",
        "for i in range(21):\n",
        "    predicted_indices = np.argmax(predictions[i], axis=-1)\n",
        "    predicted_sentence = ' '.join(tokenizer.index_word.get(idx, '') for idx in predicted_indices[0] if idx > 0)\n",
        "    predicted_sentences.append(predicted_sentence)\n",
        "\n",
        "for i, sentence in enumerate(predicted_sentences):\n",
        "    print(f\"Predicted sentence for column {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Bh8stdHZuftC"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "JhxxvoohuCW1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "pretrained_model = load_model('/content/my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdtAsEN-ucj6",
        "outputId": "4ab947fd-1460-4adf-edeb-df40b887170d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "40/40 [==============================] - 1348s 31s/step - loss: 0.6308 - decoder_dense_0_loss: 0.3469 - decoder_dense_1_loss: 0.0032 - decoder_dense_2_loss: 0.0164 - decoder_dense_3_loss: 0.0040 - decoder_dense_4_loss: 0.0095 - decoder_dense_5_loss: 0.0020 - decoder_dense_6_loss: 0.0055 - decoder_dense_7_loss: 0.0043 - decoder_dense_8_loss: 0.0057 - decoder_dense_9_loss: 0.0122 - decoder_dense_10_loss: 0.0032 - decoder_dense_11_loss: 0.0036 - decoder_dense_12_loss: 0.0014 - decoder_dense_13_loss: 0.0194 - decoder_dense_14_loss: 0.0036 - decoder_dense_15_loss: 0.0079 - decoder_dense_16_loss: 0.0042 - decoder_dense_17_loss: 0.0040 - decoder_dense_18_loss: 0.0028 - decoder_dense_19_loss: 0.0919 - decoder_dense_20_loss: 0.0170 - decoder_dense_0_accuracy: 0.9220 - decoder_dense_1_accuracy: 0.9991 - decoder_dense_2_accuracy: 0.9942 - decoder_dense_3_accuracy: 0.9986 - decoder_dense_4_accuracy: 0.9969 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9990 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9988 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9996 - decoder_dense_11_accuracy: 0.9993 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9947 - decoder_dense_14_accuracy: 0.9987 - decoder_dense_15_accuracy: 0.9977 - decoder_dense_16_accuracy: 0.9987 - decoder_dense_17_accuracy: 0.9991 - decoder_dense_18_accuracy: 0.9995 - decoder_dense_19_accuracy: 0.9716 - decoder_dense_20_accuracy: 0.9981 - val_loss: 0.7272 - val_decoder_dense_0_loss: 0.4183 - val_decoder_dense_1_loss: 0.0032 - val_decoder_dense_2_loss: 0.0166 - val_decoder_dense_3_loss: 0.0030 - val_decoder_dense_4_loss: 0.0112 - val_decoder_dense_5_loss: 0.0027 - val_decoder_dense_6_loss: 0.0032 - val_decoder_dense_7_loss: 0.0064 - val_decoder_dense_8_loss: 0.0163 - val_decoder_dense_9_loss: 0.0150 - val_decoder_dense_10_loss: 0.0030 - val_decoder_dense_11_loss: 0.0049 - val_decoder_dense_12_loss: 0.0019 - val_decoder_dense_13_loss: 0.0281 - val_decoder_dense_14_loss: 0.0048 - val_decoder_dense_15_loss: 0.0105 - val_decoder_dense_16_loss: 0.0060 - val_decoder_dense_17_loss: 0.0029 - val_decoder_dense_18_loss: 0.0022 - val_decoder_dense_19_loss: 0.0964 - val_decoder_dense_20_loss: 0.0094 - val_decoder_dense_0_accuracy: 0.9204 - val_decoder_dense_1_accuracy: 0.9991 - val_decoder_dense_2_accuracy: 0.9940 - val_decoder_dense_3_accuracy: 0.9990 - val_decoder_dense_4_accuracy: 0.9962 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9992 - val_decoder_dense_7_accuracy: 0.9981 - val_decoder_dense_8_accuracy: 0.9966 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9997 - val_decoder_dense_11_accuracy: 0.9991 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9929 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9972 - val_decoder_dense_16_accuracy: 0.9980 - val_decoder_dense_17_accuracy: 0.9993 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9706 - val_decoder_dense_20_accuracy: 0.9991 - lr: 0.0049\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 1200s 30s/step - loss: 0.6097 - decoder_dense_0_loss: 0.3352 - decoder_dense_1_loss: 0.0028 - decoder_dense_2_loss: 0.0152 - decoder_dense_3_loss: 0.0034 - decoder_dense_4_loss: 0.0091 - decoder_dense_5_loss: 0.0019 - decoder_dense_6_loss: 0.0054 - decoder_dense_7_loss: 0.0042 - decoder_dense_8_loss: 0.0052 - decoder_dense_9_loss: 0.0119 - decoder_dense_10_loss: 0.0030 - decoder_dense_11_loss: 0.0038 - decoder_dense_12_loss: 0.0012 - decoder_dense_13_loss: 0.0188 - decoder_dense_14_loss: 0.0037 - decoder_dense_15_loss: 0.0073 - decoder_dense_16_loss: 0.0041 - decoder_dense_17_loss: 0.0041 - decoder_dense_18_loss: 0.0024 - decoder_dense_19_loss: 0.0910 - decoder_dense_20_loss: 0.0158 - decoder_dense_0_accuracy: 0.9237 - decoder_dense_1_accuracy: 0.9992 - decoder_dense_2_accuracy: 0.9947 - decoder_dense_3_accuracy: 0.9989 - decoder_dense_4_accuracy: 0.9966 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9991 - decoder_dense_7_accuracy: 0.9987 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9996 - decoder_dense_11_accuracy: 0.9992 - decoder_dense_12_accuracy: 0.9998 - decoder_dense_13_accuracy: 0.9949 - decoder_dense_14_accuracy: 0.9986 - decoder_dense_15_accuracy: 0.9978 - decoder_dense_16_accuracy: 0.9989 - decoder_dense_17_accuracy: 0.9990 - decoder_dense_18_accuracy: 0.9994 - decoder_dense_19_accuracy: 0.9719 - decoder_dense_20_accuracy: 0.9983 - val_loss: 0.7096 - val_decoder_dense_0_loss: 0.4099 - val_decoder_dense_1_loss: 0.0028 - val_decoder_dense_2_loss: 0.0150 - val_decoder_dense_3_loss: 0.0027 - val_decoder_dense_4_loss: 0.0123 - val_decoder_dense_5_loss: 0.0027 - val_decoder_dense_6_loss: 0.0027 - val_decoder_dense_7_loss: 0.0060 - val_decoder_dense_8_loss: 0.0157 - val_decoder_dense_9_loss: 0.0140 - val_decoder_dense_10_loss: 0.0026 - val_decoder_dense_11_loss: 0.0048 - val_decoder_dense_12_loss: 0.0021 - val_decoder_dense_13_loss: 0.0274 - val_decoder_dense_14_loss: 0.0050 - val_decoder_dense_15_loss: 0.0096 - val_decoder_dense_16_loss: 0.0059 - val_decoder_dense_17_loss: 0.0026 - val_decoder_dense_18_loss: 0.0026 - val_decoder_dense_19_loss: 0.0954 - val_decoder_dense_20_loss: 0.0097 - val_decoder_dense_0_accuracy: 0.9227 - val_decoder_dense_1_accuracy: 0.9993 - val_decoder_dense_2_accuracy: 0.9946 - val_decoder_dense_3_accuracy: 0.9994 - val_decoder_dense_4_accuracy: 0.9960 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9994 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9991 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9934 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9977 - val_decoder_dense_16_accuracy: 0.9982 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9713 - val_decoder_dense_20_accuracy: 0.9990 - lr: 0.0049\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 1194s 30s/step - loss: 0.5994 - decoder_dense_0_loss: 0.3274 - decoder_dense_1_loss: 0.0024 - decoder_dense_2_loss: 0.0144 - decoder_dense_3_loss: 0.0030 - decoder_dense_4_loss: 0.0091 - decoder_dense_5_loss: 0.0019 - decoder_dense_6_loss: 0.0061 - decoder_dense_7_loss: 0.0040 - decoder_dense_8_loss: 0.0058 - decoder_dense_9_loss: 0.0115 - decoder_dense_10_loss: 0.0029 - decoder_dense_11_loss: 0.0034 - decoder_dense_12_loss: 0.0014 - decoder_dense_13_loss: 0.0187 - decoder_dense_14_loss: 0.0037 - decoder_dense_15_loss: 0.0072 - decoder_dense_16_loss: 0.0043 - decoder_dense_17_loss: 0.0046 - decoder_dense_18_loss: 0.0029 - decoder_dense_19_loss: 0.0909 - decoder_dense_20_loss: 0.0161 - decoder_dense_0_accuracy: 0.9251 - decoder_dense_1_accuracy: 0.9994 - decoder_dense_2_accuracy: 0.9948 - decoder_dense_3_accuracy: 0.9991 - decoder_dense_4_accuracy: 0.9969 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9989 - decoder_dense_7_accuracy: 0.9990 - decoder_dense_8_accuracy: 0.9987 - decoder_dense_9_accuracy: 0.9959 - decoder_dense_10_accuracy: 0.9996 - decoder_dense_11_accuracy: 0.9992 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9948 - decoder_dense_14_accuracy: 0.9986 - decoder_dense_15_accuracy: 0.9979 - decoder_dense_16_accuracy: 0.9986 - decoder_dense_17_accuracy: 0.9988 - decoder_dense_18_accuracy: 0.9992 - decoder_dense_19_accuracy: 0.9718 - decoder_dense_20_accuracy: 0.9980 - val_loss: 0.7648 - val_decoder_dense_0_loss: 0.4171 - val_decoder_dense_1_loss: 0.0028 - val_decoder_dense_2_loss: 0.0171 - val_decoder_dense_3_loss: 0.0024 - val_decoder_dense_4_loss: 0.0127 - val_decoder_dense_5_loss: 0.0028 - val_decoder_dense_6_loss: 0.0046 - val_decoder_dense_7_loss: 0.0050 - val_decoder_dense_8_loss: 0.0137 - val_decoder_dense_9_loss: 0.0144 - val_decoder_dense_10_loss: 0.0027 - val_decoder_dense_11_loss: 0.0047 - val_decoder_dense_12_loss: 0.0021 - val_decoder_dense_13_loss: 0.0287 - val_decoder_dense_14_loss: 0.0048 - val_decoder_dense_15_loss: 0.0127 - val_decoder_dense_16_loss: 0.0071 - val_decoder_dense_17_loss: 0.0036 - val_decoder_dense_18_loss: 0.0030 - val_decoder_dense_19_loss: 0.1007 - val_decoder_dense_20_loss: 0.0401 - val_decoder_dense_0_accuracy: 0.9220 - val_decoder_dense_1_accuracy: 0.9992 - val_decoder_dense_2_accuracy: 0.9928 - val_decoder_dense_3_accuracy: 0.9993 - val_decoder_dense_4_accuracy: 0.9959 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9992 - val_decoder_dense_7_accuracy: 0.9984 - val_decoder_dense_8_accuracy: 0.9971 - val_decoder_dense_9_accuracy: 0.9952 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9929 - val_decoder_dense_14_accuracy: 0.9984 - val_decoder_dense_15_accuracy: 0.9961 - val_decoder_dense_16_accuracy: 0.9979 - val_decoder_dense_17_accuracy: 0.9994 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9700 - val_decoder_dense_20_accuracy: 0.9934 - lr: 0.0049\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 1191s 30s/step - loss: 0.7221 - decoder_dense_0_loss: 0.3492 - decoder_dense_1_loss: 0.0032 - decoder_dense_2_loss: 0.0194 - decoder_dense_3_loss: 0.0057 - decoder_dense_4_loss: 0.0101 - decoder_dense_5_loss: 0.0021 - decoder_dense_6_loss: 0.0095 - decoder_dense_7_loss: 0.0044 - decoder_dense_8_loss: 0.0062 - decoder_dense_9_loss: 0.0119 - decoder_dense_10_loss: 0.0064 - decoder_dense_11_loss: 0.0043 - decoder_dense_12_loss: 0.0044 - decoder_dense_13_loss: 0.0234 - decoder_dense_14_loss: 0.0039 - decoder_dense_15_loss: 0.0094 - decoder_dense_16_loss: 0.0082 - decoder_dense_17_loss: 0.0077 - decoder_dense_18_loss: 0.0043 - decoder_dense_19_loss: 0.0986 - decoder_dense_20_loss: 0.0403 - decoder_dense_0_accuracy: 0.9221 - decoder_dense_1_accuracy: 0.9990 - decoder_dense_2_accuracy: 0.9931 - decoder_dense_3_accuracy: 0.9981 - decoder_dense_4_accuracy: 0.9966 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9977 - decoder_dense_7_accuracy: 0.9986 - decoder_dense_8_accuracy: 0.9985 - decoder_dense_9_accuracy: 0.9957 - decoder_dense_10_accuracy: 0.9984 - decoder_dense_11_accuracy: 0.9990 - decoder_dense_12_accuracy: 0.9991 - decoder_dense_13_accuracy: 0.9937 - decoder_dense_14_accuracy: 0.9987 - decoder_dense_15_accuracy: 0.9973 - decoder_dense_16_accuracy: 0.9975 - decoder_dense_17_accuracy: 0.9977 - decoder_dense_18_accuracy: 0.9987 - decoder_dense_19_accuracy: 0.9701 - decoder_dense_20_accuracy: 0.9907 - val_loss: 0.7858 - val_decoder_dense_0_loss: 0.4225 - val_decoder_dense_1_loss: 0.0025 - val_decoder_dense_2_loss: 0.0169 - val_decoder_dense_3_loss: 0.0059 - val_decoder_dense_4_loss: 0.0130 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0036 - val_decoder_dense_7_loss: 0.0056 - val_decoder_dense_8_loss: 0.0154 - val_decoder_dense_9_loss: 0.0143 - val_decoder_dense_10_loss: 0.0028 - val_decoder_dense_11_loss: 0.0040 - val_decoder_dense_12_loss: 0.0030 - val_decoder_dense_13_loss: 0.0292 - val_decoder_dense_14_loss: 0.0048 - val_decoder_dense_15_loss: 0.0110 - val_decoder_dense_16_loss: 0.0075 - val_decoder_dense_17_loss: 0.0041 - val_decoder_dense_18_loss: 0.0033 - val_decoder_dense_19_loss: 0.0992 - val_decoder_dense_20_loss: 0.0121 - val_decoder_dense_0_accuracy: 0.9214 - val_decoder_dense_1_accuracy: 0.9995 - val_decoder_dense_2_accuracy: 0.9944 - val_decoder_dense_3_accuracy: 0.9978 - val_decoder_dense_4_accuracy: 0.9962 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9995 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9966 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9994 - val_decoder_dense_13_accuracy: 0.9930 - val_decoder_dense_14_accuracy: 0.9982 - val_decoder_dense_15_accuracy: 0.9974 - val_decoder_dense_16_accuracy: 0.9982 - val_decoder_dense_17_accuracy: 0.9991 - val_decoder_dense_18_accuracy: 0.9993 - val_decoder_dense_19_accuracy: 0.9705 - val_decoder_dense_20_accuracy: 0.9985 - lr: 0.0049\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 1192s 30s/step - loss: 0.6457 - decoder_dense_0_loss: 0.3280 - decoder_dense_1_loss: 0.0026 - decoder_dense_2_loss: 0.0150 - decoder_dense_3_loss: 0.0036 - decoder_dense_4_loss: 0.0095 - decoder_dense_5_loss: 0.0020 - decoder_dense_6_loss: 0.0059 - decoder_dense_7_loss: 0.0044 - decoder_dense_8_loss: 0.0051 - decoder_dense_9_loss: 0.0115 - decoder_dense_10_loss: 0.0033 - decoder_dense_11_loss: 0.0033 - decoder_dense_12_loss: 0.0015 - decoder_dense_13_loss: 0.0195 - decoder_dense_14_loss: 0.0037 - decoder_dense_15_loss: 0.0069 - decoder_dense_16_loss: 0.0044 - decoder_dense_17_loss: 0.0050 - decoder_dense_18_loss: 0.0029 - decoder_dense_19_loss: 0.0918 - decoder_dense_20_loss: 0.0192 - decoder_dense_0_accuracy: 0.9242 - decoder_dense_1_accuracy: 0.9992 - decoder_dense_2_accuracy: 0.9949 - decoder_dense_3_accuracy: 0.9990 - decoder_dense_4_accuracy: 0.9968 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9990 - decoder_dense_7_accuracy: 0.9986 - decoder_dense_8_accuracy: 0.9988 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9995 - decoder_dense_11_accuracy: 0.9993 - decoder_dense_12_accuracy: 0.9997 - decoder_dense_13_accuracy: 0.9945 - decoder_dense_14_accuracy: 0.9986 - decoder_dense_15_accuracy: 0.9980 - decoder_dense_16_accuracy: 0.9987 - decoder_dense_17_accuracy: 0.9988 - decoder_dense_18_accuracy: 0.9993 - decoder_dense_19_accuracy: 0.9719 - decoder_dense_20_accuracy: 0.9969 - val_loss: 0.7469 - val_decoder_dense_0_loss: 0.4122 - val_decoder_dense_1_loss: 0.0030 - val_decoder_dense_2_loss: 0.0140 - val_decoder_dense_3_loss: 0.0025 - val_decoder_dense_4_loss: 0.0123 - val_decoder_dense_5_loss: 0.0030 - val_decoder_dense_6_loss: 0.0020 - val_decoder_dense_7_loss: 0.0051 - val_decoder_dense_8_loss: 0.0152 - val_decoder_dense_9_loss: 0.0143 - val_decoder_dense_10_loss: 0.0027 - val_decoder_dense_11_loss: 0.0041 - val_decoder_dense_12_loss: 0.0020 - val_decoder_dense_13_loss: 0.0290 - val_decoder_dense_14_loss: 0.0048 - val_decoder_dense_15_loss: 0.0099 - val_decoder_dense_16_loss: 0.0070 - val_decoder_dense_17_loss: 0.0020 - val_decoder_dense_18_loss: 0.0023 - val_decoder_dense_19_loss: 0.0985 - val_decoder_dense_20_loss: 0.0099 - val_decoder_dense_0_accuracy: 0.9225 - val_decoder_dense_1_accuracy: 0.9992 - val_decoder_dense_2_accuracy: 0.9948 - val_decoder_dense_3_accuracy: 0.9995 - val_decoder_dense_4_accuracy: 0.9964 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9998 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9968 - val_decoder_dense_9_accuracy: 0.9949 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9989 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9925 - val_decoder_dense_14_accuracy: 0.9983 - val_decoder_dense_15_accuracy: 0.9974 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9994 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9704 - val_decoder_dense_20_accuracy: 0.9986 - lr: 0.0049\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 1171s 29s/step - loss: 0.5928 - decoder_dense_0_loss: 0.3039 - decoder_dense_1_loss: 0.0019 - decoder_dense_2_loss: 0.0125 - decoder_dense_3_loss: 0.0026 - decoder_dense_4_loss: 0.0088 - decoder_dense_5_loss: 0.0018 - decoder_dense_6_loss: 0.0048 - decoder_dense_7_loss: 0.0038 - decoder_dense_8_loss: 0.0046 - decoder_dense_9_loss: 0.0114 - decoder_dense_10_loss: 0.0026 - decoder_dense_11_loss: 0.0026 - decoder_dense_12_loss: 9.8119e-04 - decoder_dense_13_loss: 0.0171 - decoder_dense_14_loss: 0.0032 - decoder_dense_15_loss: 0.0062 - decoder_dense_16_loss: 0.0043 - decoder_dense_17_loss: 0.0040 - decoder_dense_18_loss: 0.0025 - decoder_dense_19_loss: 0.0882 - decoder_dense_20_loss: 0.0156 - decoder_dense_0_accuracy: 0.9278 - decoder_dense_1_accuracy: 0.9995 - decoder_dense_2_accuracy: 0.9955 - decoder_dense_3_accuracy: 0.9993 - decoder_dense_4_accuracy: 0.9970 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9991 - decoder_dense_7_accuracy: 0.9989 - decoder_dense_8_accuracy: 0.9989 - decoder_dense_9_accuracy: 0.9958 - decoder_dense_10_accuracy: 0.9997 - decoder_dense_11_accuracy: 0.9995 - decoder_dense_12_accuracy: 0.9998 - decoder_dense_13_accuracy: 0.9952 - decoder_dense_14_accuracy: 0.9988 - decoder_dense_15_accuracy: 0.9981 - decoder_dense_16_accuracy: 0.9989 - decoder_dense_17_accuracy: 0.9989 - decoder_dense_18_accuracy: 0.9994 - decoder_dense_19_accuracy: 0.9728 - decoder_dense_20_accuracy: 0.9981 - val_loss: 0.7211 - val_decoder_dense_0_loss: 0.4040 - val_decoder_dense_1_loss: 0.0027 - val_decoder_dense_2_loss: 0.0126 - val_decoder_dense_3_loss: 0.0023 - val_decoder_dense_4_loss: 0.0111 - val_decoder_dense_5_loss: 0.0027 - val_decoder_dense_6_loss: 0.0020 - val_decoder_dense_7_loss: 0.0054 - val_decoder_dense_8_loss: 0.0127 - val_decoder_dense_9_loss: 0.0138 - val_decoder_dense_10_loss: 0.0024 - val_decoder_dense_11_loss: 0.0041 - val_decoder_dense_12_loss: 0.0017 - val_decoder_dense_13_loss: 0.0264 - val_decoder_dense_14_loss: 0.0047 - val_decoder_dense_15_loss: 0.0096 - val_decoder_dense_16_loss: 0.0068 - val_decoder_dense_17_loss: 0.0019 - val_decoder_dense_18_loss: 0.0024 - val_decoder_dense_19_loss: 0.0956 - val_decoder_dense_20_loss: 0.0088 - val_decoder_dense_0_accuracy: 0.9252 - val_decoder_dense_1_accuracy: 0.9995 - val_decoder_dense_2_accuracy: 0.9951 - val_decoder_dense_3_accuracy: 0.9996 - val_decoder_dense_4_accuracy: 0.9966 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9997 - val_decoder_dense_7_accuracy: 0.9982 - val_decoder_dense_8_accuracy: 0.9969 - val_decoder_dense_9_accuracy: 0.9950 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9995 - val_decoder_dense_13_accuracy: 0.9940 - val_decoder_dense_14_accuracy: 0.9983 - val_decoder_dense_15_accuracy: 0.9975 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9994 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9709 - val_decoder_dense_20_accuracy: 0.9988 - lr: 9.7440e-04\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 1170s 29s/step - loss: 0.5755 - decoder_dense_0_loss: 0.2971 - decoder_dense_1_loss: 0.0018 - decoder_dense_2_loss: 0.0116 - decoder_dense_3_loss: 0.0026 - decoder_dense_4_loss: 0.0084 - decoder_dense_5_loss: 0.0018 - decoder_dense_6_loss: 0.0049 - decoder_dense_7_loss: 0.0036 - decoder_dense_8_loss: 0.0040 - decoder_dense_9_loss: 0.0110 - decoder_dense_10_loss: 0.0024 - decoder_dense_11_loss: 0.0027 - decoder_dense_12_loss: 0.0010 - decoder_dense_13_loss: 0.0166 - decoder_dense_14_loss: 0.0032 - decoder_dense_15_loss: 0.0061 - decoder_dense_16_loss: 0.0036 - decoder_dense_17_loss: 0.0035 - decoder_dense_18_loss: 0.0024 - decoder_dense_19_loss: 0.0873 - decoder_dense_20_loss: 0.0141 - decoder_dense_0_accuracy: 0.9293 - decoder_dense_1_accuracy: 0.9996 - decoder_dense_2_accuracy: 0.9958 - decoder_dense_3_accuracy: 0.9992 - decoder_dense_4_accuracy: 0.9972 - decoder_dense_5_accuracy: 0.9996 - decoder_dense_6_accuracy: 0.9990 - decoder_dense_7_accuracy: 0.9989 - decoder_dense_8_accuracy: 0.9990 - decoder_dense_9_accuracy: 0.9959 - decoder_dense_10_accuracy: 0.9996 - decoder_dense_11_accuracy: 0.9995 - decoder_dense_12_accuracy: 0.9998 - decoder_dense_13_accuracy: 0.9952 - decoder_dense_14_accuracy: 0.9987 - decoder_dense_15_accuracy: 0.9982 - decoder_dense_16_accuracy: 0.9991 - decoder_dense_17_accuracy: 0.9992 - decoder_dense_18_accuracy: 0.9995 - decoder_dense_19_accuracy: 0.9732 - decoder_dense_20_accuracy: 0.9984 - val_loss: 0.7145 - val_decoder_dense_0_loss: 0.4013 - val_decoder_dense_1_loss: 0.0026 - val_decoder_dense_2_loss: 0.0131 - val_decoder_dense_3_loss: 0.0023 - val_decoder_dense_4_loss: 0.0116 - val_decoder_dense_5_loss: 0.0027 - val_decoder_dense_6_loss: 0.0019 - val_decoder_dense_7_loss: 0.0054 - val_decoder_dense_8_loss: 0.0128 - val_decoder_dense_9_loss: 0.0141 - val_decoder_dense_10_loss: 0.0024 - val_decoder_dense_11_loss: 0.0040 - val_decoder_dense_12_loss: 0.0016 - val_decoder_dense_13_loss: 0.0265 - val_decoder_dense_14_loss: 0.0045 - val_decoder_dense_15_loss: 0.0093 - val_decoder_dense_16_loss: 0.0066 - val_decoder_dense_17_loss: 0.0019 - val_decoder_dense_18_loss: 0.0020 - val_decoder_dense_19_loss: 0.0951 - val_decoder_dense_20_loss: 0.0087 - val_decoder_dense_0_accuracy: 0.9253 - val_decoder_dense_1_accuracy: 0.9996 - val_decoder_dense_2_accuracy: 0.9948 - val_decoder_dense_3_accuracy: 0.9994 - val_decoder_dense_4_accuracy: 0.9966 - val_decoder_dense_5_accuracy: 0.9995 - val_decoder_dense_6_accuracy: 0.9996 - val_decoder_dense_7_accuracy: 0.9983 - val_decoder_dense_8_accuracy: 0.9970 - val_decoder_dense_9_accuracy: 0.9951 - val_decoder_dense_10_accuracy: 0.9998 - val_decoder_dense_11_accuracy: 0.9992 - val_decoder_dense_12_accuracy: 0.9996 - val_decoder_dense_13_accuracy: 0.9940 - val_decoder_dense_14_accuracy: 0.9984 - val_decoder_dense_15_accuracy: 0.9976 - val_decoder_dense_16_accuracy: 0.9983 - val_decoder_dense_17_accuracy: 0.9994 - val_decoder_dense_18_accuracy: 0.9995 - val_decoder_dense_19_accuracy: 0.9708 - val_decoder_dense_20_accuracy: 0.9988 - lr: 9.7440e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79638f04f220>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrained_model.fit([X_train, y_train], [y_train[:, i, :] for i in range(y_train.shape[1])], epochs=20, batch_size= 32,\n",
        "             validation_split=0.2, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "f_igeAu2Q9-E"
      },
      "outputs": [],
      "source": [
        "pretrained_model.save('my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sune-oBRDId",
        "outputId": "3210edeb-f06a-4e69-a08e-f9fb9a3b9d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Predicted sentence for column 1: 1 report surgery amp medicines the history history acute medical medical mild mild findings findings\n",
            "Predicted sentence for column 2: no inj\n",
            "Predicted sentence for column 3: prefer procedure calculus propofol in meals operative\n",
            "Predicted sentence for column 4: after medical ho\n",
            "Predicted sentence for column 5: 7 inj\n",
            "Predicted sentence for column 6: cholecystectomy up\n",
            "Predicted sentence for column 7: 3 2 with months days happen\n",
            "Predicted sentence for column 8: alcohol smoking\n",
            "Predicted sentence for column 9: 2 with anes anesthic\n",
            "Predicted sentence for column 10: recommende cholelithiasis\n",
            "Predicted sentence for column 11: oil worker date past check drink mustrad\n",
            "Predicted sentence for column 12: oil treatment smooth treatment\n",
            "Predicted sentence for column 13: recovery follow does\n",
            "Predicted sentence for column 14: attack follow ago\n",
            "Predicted sentence for column 15: type\n",
            "Predicted sentence for column 16: type\n",
            "Predicted sentence for column 17: complains follow patient\n",
            "Predicted sentence for column 18: since it abdomen\n",
            "Predicted sentence for column 19: gb abdomen\n",
            "Predicted sentence for column 20: not problem normal region 5 other\n",
            "Predicted sentence for column 21: habit problem lap drug light general have pain\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted sentence for column 1: vomiting 10 both acute acute acute acute history 1 surgery surgery surgery surgery biopsy biopsy biopsy biopsy biopsy\n",
            "Predicted sentence for column 2: type inj\n",
            "Predicted sentence for column 3: prefer procedure in suggested operative\n",
            "Predicted sentence for column 4: after medical ho\n",
            "Predicted sentence for column 5: 7 inj\n",
            "Predicted sentence for column 6: cholecystectomy up\n",
            "Predicted sentence for column 7: 3 2 with months days happen\n",
            "Predicted sentence for column 8: alcohol smoking\n",
            "Predicted sentence for column 9: 2 with anes anesthic\n",
            "Predicted sentence for column 10: recommende cholelithiasis\n",
            "Predicted sentence for column 11: oil worker date past check drink mustrad\n",
            "Predicted sentence for column 12: oil treatment smooth treatment\n",
            "Predicted sentence for column 13: recovery follow does\n",
            "Predicted sentence for column 14: attack follow year\n",
            "Predicted sentence for column 15: type\n",
            "Predicted sentence for column 16: type\n",
            "Predicted sentence for column 17: complains follow patient\n",
            "Predicted sentence for column 18: since it abdomen\n",
            "Predicted sentence for column 19: gb abdomen\n",
            "Predicted sentence for column 20: not problem normal january cholecystitis on\n",
            "Predicted sentence for column 21: habit problem general have pain general\n"
          ]
        }
      ],
      "source": [
        "input_sentence_list = [\"Ho intermittent pain in epigastric region since one year. It was associated with vomiting  flatulence . Abdomen problem . Complains from less than 1 year ago . Gradually it happen . Moderate problem . No attack . Patient does not have smoking habit . Light Worker . Does not drink alcohol . Non Vegetarian . Prefer meals 2 3 times a day . Prefer oil mustrad oil . Past medical history Surgery uterus tumour in 1991 . Past medical history Hypertensive  DM II on treatment . Medicine refered by other TAB. AMLOZ INJ. INSULIN . Acute resolving calculus cholecystitis Gangreenous with empyema . Treatment type OPERATIVE . Surgery type CHOLECYSTECTOMY . Procedure follow CHOLECYSTECTOMY   LAP . Anesthic type General . Anes smooth recovery . Recommende drug INJ. PYROLATE 1 AMP INJ. PROPOFOL .  INJ. ROCURONIUM . Findings  GB was grossly distended filled with pus Friable hartman s pouch Empyema GB Moderate large size calculi present Calot s area grossly edematous CBD normal . NORMAL recovery . Medicines suggested Inj.Tazar 4.5 gmInj.Ketanav 2 AMPInj.Pantocid 40 mg . Follow-up date 21 March 2013 . CHECK UP WITH BIOPSY REPORT AFTER 7 DAYS . \",\n",
        "                  \"loose stool mixed with blood melaena since one month. irregular bowel. sometimes he passed stool thrice daily. ho anaemia, acute gastritis  uti one month ago. open gb 1994 . Complains from 1 month ago . Gradually it happen . Moderate problem . No attack . Patient does not have smoking habit . Moderate Worker . Does not drink alcohol . Both VegNon Veg . Prefer meals 2 3 times a day . Prefer oil mustrad oil . WELL DIFFERENTIAT ADENOCARCINOMA RECTUM UPPER 3RD . Treatment type OPERATIVE . Surgery type MULTIPLE . Procedure follow lap assested anterior resection  stappled anastomosis . Anesthic type General . Anes smooth recovery . Recommende drug INJ. PYROLATE 1 AMP .  INJ. PROPOFOL . NORMAL recovery . Medicines suggested Normal 0 false false false EN IN X NONE X NONE MicrosoftInternetExplorer4 . Follow-up date 03 July 2013 . CHECK UP AFTER 7 DAYS . \"]\n",
        "for input_sentence in input_sentence_list:\n",
        "  input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "  input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "  decoder_input_sequence = np.zeros((1, 21, y_max_sequence_length))\n",
        "\n",
        "  predictions = pretrained_model.predict([input_sequence, decoder_input_sequence])\n",
        "\n",
        "  predicted_sentences = []\n",
        "  for i in range(21):\n",
        "      predicted_indices = np.argmax(predictions[i], axis=-1)\n",
        "      predicted_sentence = ' '.join(tokenizer.index_word.get(idx, '') for idx in predicted_indices[0] if idx > 0)\n",
        "      predicted_sentences.append(predicted_sentence)\n",
        "\n",
        "  for i, sentence in enumerate(predicted_sentences):\n",
        "      print(f\"Predicted sentence for column {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KflNDL5pcvQT"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n",
        "# # Fit the model\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# history = model.fit([X_train, y_train],  # Inputs are a list: [encoder_inputs, decoder_inputs]\n",
        "#             [y_train[:, i, :] for i in range(y_train.shape[1])],  # Outputs are a list of decoder outputs\n",
        "#             epochs=50,      ## 150 + 40 + 20 + 20\n",
        "#             batch_size=16,\n",
        "#             callbacks=[early_stopping, reduce_lr],\n",
        "#             validation_data=([X_test, y_test],  # Validation inputs\n",
        "#                             [y_test[:, i, :] for i in range(y_test.shape[1])]))  # Validation outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wkz1BUz0jveU"
      },
      "outputs": [],
      "source": [
        "# # Assuming evaluation_inputs and y_test are correctly formatted\n",
        "# evaluation_inputs = [X_test, y_test]\n",
        "\n",
        "# # Evaluate the model\n",
        "# losses = model.evaluate(evaluation_inputs,  # Input should match [encoder_inputs, decoder_inputs]\n",
        "#                         [y_test[:, i, :] for i in range(y_test.shape[1])],  # Output should match decoder outputs\n",
        "#                         batch_size=16)\n",
        "\n",
        "# # Print evaluation results\n",
        "# for i, metric_name in enumerate(model.metrics_names):\n",
        "#     print(f'Test {metric_name}: {losses[i]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZbyn4i_shx-"
      },
      "outputs": [],
      "source": [
        "# # Example input sentence and preprocessing\n",
        "# input_sentence = \"Sophia Martinez, 38-year-old female nurse, visits on July 5, 2024. Complains of chronic back pain and fatigue. Describes it as a dull ache in the lower back. History of anemia. No surgical history. Father had osteoporosis. Advise DEXA scan and blood tests. Prescribe iron supplements and recommend physical therapy. Follow-up appointment in six weeks.\"\n",
        "# input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "# input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# # Prepare decoder input sequence\n",
        "# decoder_input_sequence = np.zeros((1, 31, y_max_sequence_length))  # Assuming y_max_sequence_length is 200\n",
        "\n",
        "# # Predict using the model\n",
        "# predictions = model.predict([input_sequence, decoder_input_sequence])\n",
        "\n",
        "# # Process predictions\n",
        "# predicted_sentences = []\n",
        "# for i in range(31):\n",
        "#     predicted_indices = np.argmax(predictions[i], axis=-1)\n",
        "#     predicted_sentence = ' '.join(tokenizer.index_word.get(idx, '') for idx in predicted_indices[0] if idx > 0)\n",
        "#     predicted_sentences.append(predicted_sentence)\n",
        "\n",
        "# # Print predicted sentences\n",
        "# for i, sentence in enumerate(predicted_sentences):\n",
        "#     print(f\"Predicted sentence for column {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2vDXdoUSUFd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ2i69JlcmL0"
      },
      "outputs": [],
      "source": [
        "# encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
        "# decoder_state_input_h = Input(shape=(latent_dim * 2,), name='decoder_state_input_h')\n",
        "# decoder_state_input_c = Input(shape=(latent_dim * 2,), name='decoder_state_input_c')\n",
        "\n",
        "# decoder_inputs_single = Input(shape=(1,), name='decoder_inputs_single')\n",
        "# decoder_embedding_single = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, weights=[embedding_matrix_input], trainable=False)(decoder_inputs_single)\n",
        "\n",
        "# decoder_lstm_output, decoder_state_h, decoder_state_c = LSTM(latent_dim * 2, return_sequences=True, return_state=True, dropout=0.5, recurrent_dropout=0.5)(\n",
        "#     decoder_embedding_single, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# decoder_dense_single = Dense(vocab_size_input, activation='softmax')\n",
        "# decoder_outputs_single = decoder_dense_single(decoder_lstm_output)\n",
        "\n",
        "# decoder_model = Model(\n",
        "#     [decoder_inputs_single, decoder_state_input_h, decoder_state_input_c],\n",
        "#     [decoder_outputs_single, decoder_state_h, decoder_state_c])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJeygxA9o1mx"
      },
      "outputs": [],
      "source": [
        "# tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ojFy4b-pcmZP"
      },
      "outputs": [],
      "source": [
        "# def predict_all_sentences(input_seq):\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "#     all_decoded_sentences = []\n",
        "\n",
        "#     for i in range(31):\n",
        "#         decoded_sentence = ''\n",
        "#         target_seq = np.zeros((1, 1))\n",
        "#         target_seq[0, 0] = start_token_id\n",
        "#         stop_condition = False\n",
        "\n",
        "#         while not stop_condition:\n",
        "#             output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "#             sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#             sampled_char = tokenizer.index_word[sampled_token_index]\n",
        "#             decoded_sentence += ' ' + sampled_char\n",
        "\n",
        "#             # if (sampled_char == 'END' or len(decoded_sentence) > 200):\n",
        "#             if (sampled_char == '<end>' or len(decoded_sentence) > 200):\n",
        "#                 stop_condition = True\n",
        "\n",
        "#             target_seq = np.zeros((1, 1))\n",
        "#             target_seq[0, 0] = sampled_token_index\n",
        "#             states_value = [h, c]\n",
        "\n",
        "#         all_decoded_sentences.append(decoded_sentence)\n",
        "\n",
        "#     return all_decoded_sentences\n",
        "\n",
        "# # Example usage:\n",
        "# input_seq = X_test[0:1]  # Assuming X_test is prepared similar to X_train\n",
        "# predicted_sentences = predict_all_sentences(input_seq)\n",
        "# print(predicted_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sshmF01vcmcF"
      },
      "outputs": [],
      "source": [
        "# X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OUUlHRVlsW7"
      },
      "outputs": [],
      "source": [
        "# predictions[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjuInxQWnGzG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
