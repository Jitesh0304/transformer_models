{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, GPT2Config, Trainer, TrainingArguments, AdamW, DataCollatorWithPadding\n",
    "from transformers.optimization import get_scheduler\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\jites\\Desktop\\new_folder\\synthetic_patient_data_with_lab_results.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Prepare input text and labels\n",
    "df['input_text'] = df.apply(lambda x: f\"Age: {x['Age']} Gender: {x['Gender']} Symptoms: {x['Symptoms']} History: {x['Illness History']} LabTests: {x['Lab Test Results']}\", axis=1)\n",
    "df['Diagnosis'] = df['Diagnosis Suggested']\n",
    "\n",
    "labels = df['Diagnosis'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'], labels, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 84 Gender: Female Symptoms: Hot Flashes, Muscle Cramps History: Leg Weakness, Strong Urine Odor LabTests: HR: 97 bpm, BP: 103/78 mmHg, RR: 13, O2 Sat: 96%, Temp: 38.3Â°C, Amylase: 45 U/L, Lipase: 459 U/L, WBC: (72.0, 0) lakhs/L, Na: 142 mmol/L, ALT: 17 U/L\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = df_train['text']\n",
    "# y_train = df_train['label']\n",
    "# x_test = df_test['text']\n",
    "# y_test = df_test['label']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = torch.tensor(label_encoder.fit_transform(train_labels))\n",
    "y_test_encoded = torch.tensor(label_encoder.transform(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.num_labels = len(label_encoder.classes_) \n",
    "\n",
    "# Load tokenizer and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=config).to(device)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # For consistent padding to the right\n",
    "\n",
    "model.config.attn_pdrop = 0.2\n",
    "model.config.embd_pdrop = 0.2\n",
    "model.config.resid_pdrop = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_encodings = tokenize(train_texts.tolist())\n",
    "test_encodings = tokenize(val_texts.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.encodings.items():\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                item[key] = val[idx].clone().detach()\n",
    "            else:\n",
    "                item[key] = torch.tensor(val[idx])\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, y_train_encoded)\n",
    "test_dataset = CustomDataset(test_encodings, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create datasets\n",
    "# train_dataset = Dataset.from_dict({'input_text': train_texts, 'labels': train_labels})\n",
    "# val_dataset = Dataset.from_dict({'input_text': val_texts, 'labels': val_labels})\n",
    "\n",
    "\n",
    "# # Add a padding token\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # Define a new padding token\n",
    "\n",
    "# # Define a padding token\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Use the end of text token as the padding token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id  # Use the end of text token as the padding token\n",
    "\n",
    "# model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=len(df['Diagnosis'].unique()))\n",
    "# model.resize_token_embeddings(len(tokenizer))  # Resize token embeddings to account for new padding token\n",
    "# model.to(device)\n",
    "\n",
    "# # Tokenization function\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['input_text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# # Tokenize datasets\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/280 [00:00<?, ?it/s]c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                \n",
      " 10%|â–ˆ         | 28/280 [00:31<04:23,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2788782119750977, 'eval_runtime': 0.982, 'eval_samples_per_second': 101.835, 'eval_steps_per_second': 13.239, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 56/280 [01:03<03:56,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.265268564224243, 'eval_runtime': 0.9897, 'eval_samples_per_second': 101.04, 'eval_steps_per_second': 13.135, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 84/280 [01:36<03:28,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4272756576538086, 'eval_runtime': 0.9929, 'eval_samples_per_second': 100.717, 'eval_steps_per_second': 13.093, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 100/280 [01:54<03:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.431, 'grad_norm': 4.4741692543029785, 'learning_rate': 0.0001785714285714286, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/280 [02:09<02:52,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.480830669403076, 'eval_runtime': 1.0289, 'eval_samples_per_second': 97.192, 'eval_steps_per_second': 12.635, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 113/280 [02:11<03:14,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 131.4705, 'train_samples_per_second': 68.456, 'train_steps_per_second': 2.13, 'train_loss': 3.415597223602565, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt_diagnosis_tokenizer\\\\tokenizer_config.json',\n",
       " './gpt_diagnosis_tokenizer\\\\special_tokens_map.json',\n",
       " './gpt_diagnosis_tokenizer\\\\vocab.json',\n",
       " './gpt_diagnosis_tokenizer\\\\merges.txt',\n",
       " './gpt_diagnosis_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # Accumulate over 4 batches to simulate a batch size of 32\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=200,\n",
    "    save_steps=300,\n",
    "    warmup_steps=300,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=3,\n",
    "    eval_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                            num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./gpt_diagnosis_model')\n",
    "tokenizer.save_pretrained('./gpt_diagnosis_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output disease=>  tensor([19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "label_encoder = LabelEncoder()\n",
    "# def predict(texts):\n",
    "#     encodings = tokenize(texts)\n",
    "#     outputs = model(**encodings).to(device)\n",
    "#     predictions = torch.argmax(outputs.logits, dim=1)\n",
    "#     return label_encoder.inverse_transform(predictions.tolist())\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt_diagnosis_tokenizer')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('./gpt_diagnosis_model').to(device)       ##, config=config\n",
    "\n",
    "# def tokenize(texts):\n",
    "#     return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "def predict(texts):\n",
    "    # Tokenize and move input tensors to the correct device\n",
    "    # encodings = tokenize(texts)\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Move outputs to CPU for processing\n",
    "    logits = outputs.logits.cpu()\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    top_values, top_indices = torch.topk(logits, k=3)\n",
    "\n",
    "    # js_data = open(r'C:\\Users\\jites\\Desktop\\Project_folder\\Jupyter_practice\\diesease_data_reverse.json')\n",
    "    # json_data = json.load(js_data)\n",
    "\n",
    "    # predictions = json_data.get(str(int(predictions[-1])), 'Unknown disease')\n",
    "\n",
    "    return predictions\n",
    "    # return label_encoder.inverse_transform(predictions.tolist())\n",
    "    #### return top_values, top_indices, logits\n",
    "\n",
    "predictions = predict([\"Age: 84 Gender: Female Symptoms: Hot Flashes, Muscle Cramps History: Leg Weakness, Strong Urine Odor LabTests: HR: 97 bpm, BP: 103/78 mmHg, RR: 13, O2 Sat: 96%, Temp: 38.3Â°C, Amylase: 45 U/L, Lipase: 459 U/L, WBC: (72.0, 0) lakhs/L, Na: 142 mmol/L, ALT: 17 U/L\"])\n",
    "\n",
    "print(\"Output disease=> \", predictions)\n",
    "\n",
    "\n",
    "\n",
    "# def get_probabilities(logits):\n",
    "#     return softmax(logits, dim=-1)\n",
    "\n",
    "# def calculate_top_k_accuracy(predictions, labels):\n",
    "#     top_values, top_indices = get_top_k_predictions(predictions, k=k)\n",
    "#     probabilities = get_probabilities(predictions)\n",
    "    \n",
    "\n",
    "#     js_data = open(r'C:\\Users\\jites\\Desktop\\Project_folder\\Jupyter_practice\\diesease_data_reverse.json')\n",
    "#     json_data = json.load(js_data)\n",
    "\n",
    "\n",
    "#     correct = 0\n",
    "#     for i in range(len(labels)):\n",
    "#         true_label = labels[i].item()\n",
    "#         top_k_indices = top_indices[i].tolist()\n",
    "#         top_k_probabilities = probabilities[i][top_k_indices].tolist()\n",
    "        \n",
    "#         if true_label in top_k_indices:\n",
    "#             correct += 1\n",
    "    \n",
    "#     accuracy = correct / len(labels)\n",
    "#     return top_values, top_indices, probabilities, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2-diagnosis-model').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-diagnosis-tokenizer')\n",
    "# Set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enable mixed precision if using a compatible GPU\n",
    "model = model.half() if torch.cuda.is_available() else model\n",
    "\n",
    "# Example of a generation function using a pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def generate_labels(sent):\n",
    "    paragraph = sent\n",
    "    input_text = f\"{paragraph} =>\"\n",
    "\n",
    "    # Encode with truncation and padding\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=256,  # Adjust based on your requirement\n",
    "        truncation=True,  # Explicit truncation\n",
    "        padding='max_length'  # Padding to the max length\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Generate predictions with optimized settings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # max_new_tokens=32,\n",
    "            max_length=288,  # 512\n",
    "            no_repeat_ngram_size=3,  # Prevent repeating 3-grams\n",
    "            repetition_penalty=2.0,  # Penalize repetition\n",
    "            num_return_sequences=1,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Process and return the output\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(output_text)\n",
    "    labels = output_text.split('=>')\n",
    "    if len(labels) >= 2:\n",
    "        labels = labels[1].strip()\n",
    "    return labels\n",
    "\n",
    "\n",
    "# sent = \"Age: 48 Gender: Male Symptoms: Episodes of fainting, heart palpitations, and sweating IllnessHistory: Chest discomfort started suddenly after physical exertion, radiating to the left arm, with associated sweating. VitalSigns: BP 160/100, HR 110 bpm, RR 24, O2 Sat 90%, Temp 37.5Â°C LabResults: Troponin 0.22 ng/mL, WBC 12.5 x10^9/L, Na 138 mmol/L, K 4.1 mmol/L, CRP 30 mg/L ImagingResults: Ultrasound abdomen ! Gallbladder stones with signs of cholecystitis. AdditionalInfo: Chronic alcohol consumption for 10 years, with elevated liver enzymes.\"\n",
    "sent = \"Age: 44 Gender: Male Symptoms: Joint pain and stiffness, especially in the morning IllnessHistory: Frequent urination and excessive thirst for 2 weeks, blurred vision when reading small print. VitalSigns: BP 135/85, HR 95 bpm, RR 20, O2 Sat 96%, Temp 37.0Â°C LabResults: D-dimer 0.85 ug/mL, WBC 11.0 x10^9/L, Na 137 mmol/L, BNP 350 pg/mL ImagingResults: CT chest ! Large pulmonary embolism in the right lung. AdditionalInfo: Patient has a family history of diabetes and heart disease.\"\n",
    "# sent = \"Age: 50 Gender: Female Symptoms: Dizziness, blurred vision IllnessHistory: Chronic cough for the past 3 weeks, with significant weight loss. VitalSigns: BP 130/85, HR 100 bpm, RR 22, O2 Sat 98% LabResults: CRP 15 mg/L, WBC 15.5 x10^9/L, ESR 50 mm/h ImagingResults: MRI lumbar spine ! Herniated disc at L4-L5. AdditionalInfo: nan\" ##  => Severe UTI\n",
    "# sent = \"Age: 34 Gender: Female Symptoms: Severe lower back pain radiating to legs IllnessHistory: Frequent urination and excessive thirst for 2 weeks, blurred vision when reading small print. VitalSigns: BP 145/95, HR 100 bpm, RR 18, O2 Sat 93 percent, Temp 38.0Â°C LabResults: Amylase 520 U per liter, Lipase 720 U per liter, WBC 14.0 x10^9 per liter, Na 135 mmol per liter, K 4.2 mmol per liter, ALT 75 U per liter ImagingResults: MRI brain ! Small ischemic infarct in the right parietal lobe. AdditionalInfo: Chronic alcohol consumption for 10 years, with elevated liver enzymes.\"  ## => Stroke (Ischemic)\n",
    "\n",
    "result = generate_labels(sent)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
