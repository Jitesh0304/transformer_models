{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-One Sequence Problems with a Single Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In many-to-one sequence problems, each input sample has more than one time-step, however the output consists of a single element. \n",
    "Each time-step in the input can have one or more features.\n",
    "\n",
    "Let's first create the dataset. Our dataset will consist of 15 samples. Each sample will have 3 time-steps where each time-step will \n",
    "consist of a single feature i.e. a number. The output for each sample will be the sum of the numbers in each of the three time-steps. \n",
    "For instance, if our sample contains a sequence 4,5,6 the output will be 4 + 5 + 6 = 10.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X = np.array([x+1 for x in range(45)])\n",
    "print(X)\n",
    "\n",
    "X = X.reshape(15,3,1)\n",
    "print(X)\n",
    "\"\"\"\n",
    "[[[ 1]\n",
    "  [ 2]\n",
    "  [ 3]]\n",
    "\n",
    " [[ 4]\n",
    "  [ 5]\n",
    "  [ 6]]\n",
    "  .\n",
    "  .\n",
    "  ]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "each element in the output will be equal to the sum of the values in the time-steps in the corresponding input sample.\n",
    "\"\"\"\n",
    "Y = list()\n",
    "for x in X:\n",
    "    Y.append(x.sum())\n",
    "\n",
    "Y = np.array(Y)\n",
    "print(Y)\n",
    "\n",
    "\"\"\"\n",
    "[  6  15 . . . . . ]\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(3, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "\n",
    "test_input = array([50,51,52])\n",
    "test_input = test_input.reshape((1, 3, 1))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(3, 1)))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(25, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bidirectional LSTM is a type of LSTM which learns from the input sequence from both forward and backward directions. \n",
    "The final sequence interpretation is the concatenation of both forward and backward learning passes.\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-one Sequence Problems with Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In a many-to-one sequence problem we have an input where each time-steps consists of multiple features. The output can be a single value \n",
    "or multiple values, one per feature in the input time step.\n",
    "\n",
    "dataset will contain 15 samples. Each sample will consist of 3 time-steps. Each time-steps will have two features.\n",
    "\n",
    "Let's create two lists. One will contain multiples of 3 until 135 i.e. 45 elements in total. The second list will contain multiples of 5,\n",
    "from 1 to 225. The second list will also contain 45 elements in total.\n",
    "\"\"\"\n",
    "\n",
    "X1 = np.array([x+3 for x in range(0, 135, 3)])\n",
    "print(X1)\n",
    "\n",
    "X2 = np.array([x+5 for x in range(0, 225, 5)])\n",
    "print(X2)\n",
    "\n",
    "\"\"\"\n",
    "The aggregated dataset can be created by joining the two lists\n",
    "\"\"\"\n",
    "X = np.column_stack((X1, X2))\n",
    "print(X)\n",
    "\n",
    "\"\"\"\n",
    "We need to reshape our data into three dimensions so that it can be used by LSTM. We have 45 rows in total and two columns in our dataset. \n",
    "We will reshape our dataset into 15 samples, 3 time-steps\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The output will also have 15 values corresponding to 15 input samples. Each value in the output will be the sum of the two feature values in \n",
    "the third time-step of each input sample. For instance, the third time-step of the first sample has features 9 and 15, hence the output will \n",
    "be 24. Similarly, the two feature values in the third time-step of the 2nd sample are 18 and 30; the corresponding output will be 48, and so on.\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(3, 2)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "\n",
    "test_input = array([[8, 51],\n",
    "                    [11,56],\n",
    "                    [14,61]])\n",
    "\n",
    "test_input = test_input.reshape((1, 3, 2))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution via Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(3, 2)))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(25, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution via Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 2)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Till now we have predicted single values based on multiple features values from different time-steps. There is another case of many-to-one \n",
    "sequences where you want to predict one value for each feature in the time-step. For instance, the dataset we used in this section has three \n",
    "time-steps and each time-step has two features. We may want to predict individual values for each feature series.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "[[[  3   5]\n",
    "  [  6  10]\n",
    "  [  9  15]]\n",
    "In the output, we want one time-step with two features as shown below:\n",
    "\n",
    "[12, 20]\n",
    "\n",
    "the first value in the output is a continuation of the first series and the second value is the continuation of the second series. \n",
    "We can solve such problems by simply changing the number of neurons in the output dense layer to the number of features values that \n",
    "we want in the output. However, first we need to update our output vector Y. The input vector will remain the same\n",
    "\"\"\"\n",
    "\n",
    "Y = list()\n",
    "for x in X:\n",
    "    new_item = list()\n",
    "    new_item.append(x[2][0]+3)\n",
    "    new_item.append(x[2][1]+5)\n",
    "    Y.append(new_item)\n",
    "\n",
    "Y = np.array(Y)\n",
    "print(Y)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(3, 2)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "\n",
    "test_input = array([[20,34],\n",
    "                    [23,39],\n",
    "                    [26,44]])\n",
    "\n",
    "test_input = test_input.reshape((1, 3, 2))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(3, 2)))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(25, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=500, validation_split=0.2, verbose=1)\n",
    "\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 2)))\n",
    "model.add(Dense(2))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
